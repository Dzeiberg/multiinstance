{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp gradientMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Based Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\alpha_i}$: the local dictCurve estimate for the $i^{th}$ bag\n",
    "\n",
    "$\\hat{\\alpha_{c_i}}$: the $i^{th}$ global distCurve estimate using bootstrapped sample\n",
    "\n",
    "$w_{ji}$: the contribution of bag j to the $i^{th}$ global estimate\n",
    "\n",
    "$\\tilde{\\alpha_i}$: the expected global class prior given the current contribution values and local estimates for each bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{\\alpha_i} = \\frac{w_{1i} \\cdot \\hat{\\alpha_1} \\cdot n_1 \\dots w_{Ni} \\cdot \\hat{\\alpha_N} \\cdot n_N}{w_{1i} \\cdot n_1 \\dots w_{Ni} \\cdot n_N} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss for cluster $c_i$\n",
    "\n",
    "\n",
    "$\\mathcal{L}_{c_i} = \\frac{1}{2}(\\tilde{\\alpha_i} - \\hat{\\alpha_{c_i}})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def gradientMethod(ds):\n",
    "        alphaHat : init alphaHat for each bag\n",
    "        alpha_C : get K global alpha estimates\n",
    "        init W randomly\n",
    "        for each iteration:\n",
    "            # calcualte loss given the current values of alphaHat and w\n",
    "            loss = lossFunction(w[:,1], alpha_C[1]) + ... + lossFunction(w[:,K], alpha_C[K])\n",
    "            # update alphaHat\n",
    "            alphaHat = alphaHat - eta * grad(loss)\n",
    "            # calculate the loss give the current w and new alphaHats\n",
    "            loss = lossFunction(1) + ... + lossFunction(K)\n",
    "            w = w - eta * grad(loss)\n",
    "            getMAE(alphaHat, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import autograd.scipy.stats as agss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from multiinstance.dataset_utils import buildDataset\n",
    "from multiinstance.utils import *\n",
    "from multiinstance.distanceApproaches import *\n",
    "from multiinstance.agglomerative_clustering import AgglomerativeClustering\n",
    "from numba import set_num_threads\n",
    "\n",
    "import scipy.stats as ss\n",
    "from multiinstance.data.realData import buildDataset as getRealDS\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_num_threads(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bimodal():\n",
    "    if np.random.binomial(1,.5):\n",
    "        return np.random.beta(2,10)\n",
    "    return np.random.beta(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def getAlphaHat(dsi,reps=10):\n",
    "    P, U = list(zip(*[dsi.getBag(int(i)) for i in range(dsi.N)]))\n",
    "    p = np.concatenate(P)\n",
    "    u = np.concatenate(U)\n",
    "    alphaHats,_ = getEsts(p,u,reps)\n",
    "    return alphaHats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initDS(ds_size=100,n_alpha_ests=50, nP=None, nU=None,\n",
    "           alphaDistr=lambda: np.random.uniform(0.1,.5),posMean=None, negMean=None,cov=None):\n",
    "    dsi = buildDataset(ds_size,alphaDistr=alphaDistr, nP=nP,\n",
    "                       nU=nU,posMean=posMean, negMean=negMean,cov=cov)\n",
    "\n",
    "#     dsi = addTransformScores(dsi)\n",
    "    dsi.alphaHats,dsi.curves = getBagAlphaHats(dsi,numbootstraps=n_alpha_ests)\n",
    "    dsi.globalAlphaHats = getAlphaHat(dsi,reps=n_alpha_ests)\n",
    "    return dsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def getAlphaLoss(w,n, alphaHats):\n",
    "    def loss(localAlphaHats):\n",
    "        lossVal = 0\n",
    "        for wi, aH in zip(w, alphaHats):\n",
    "            tilde = (1 / np.dot(wi,n)) * np.dot(np.multiply(localAlphaHats,wi),n)\n",
    "            lossVal = lossVal + .5 * np.square(aH - tilde)\n",
    "        return lossVal\n",
    "    return loss\n",
    "    \n",
    "def getWLoss(a,n, alphaHats, regLambda=1e-5):\n",
    "    def loss(w):\n",
    "        lossVal = 0\n",
    "        for wi,aH in zip(w, alphaHats):\n",
    "            den = (1 / np.dot(wi,n))\n",
    "            aXw = np.multiply(a,wi)\n",
    "            dot = np.dot(aXw,n)\n",
    "            tilde =  den * dot\n",
    "            lossVal = lossVal + .5 * np.square(aH - tilde)\n",
    "        lossVal = lossVal + regLambda * np.linalg.norm(w)\n",
    "        return lossVal\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0,high=5,size=(2,3)) - np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aL0(w,n,globalAlphaHats,a0):\n",
    "    def loss(localAlphaHats):\n",
    "        lossVal = 0\n",
    "#         localAlphaHats = 1 / (1 + np.exp(-1 * localAlphaHats))\n",
    "        for wi, aH in zip(w, globalAlphaHats):\n",
    "            tilde = 1 / np.sum(np.multiply(n,wi))\n",
    "            wiXA = np.multiply(wi,localAlphaHats)\n",
    "            tilde = tilde * np.sum(np.multiply(wiXA,\n",
    "                                               n))\n",
    "            lossVal = lossVal + .5 * np.square(aH - tilde)\n",
    "        lossVal = lossVal + .1 * np.sum(np.var(localAlphaHats,axis=1))\n",
    "        lossVal = lossVal + .1 * np.sum(np.square(localAlphaHats - a0))\n",
    "        return lossVal\n",
    "    return loss\n",
    "\n",
    "def wL0(localAlphaHats, n, globalAlphaHats,regLambda=0):\n",
    "#     localAlphaHats = 1 / (1 + np.exp(-1 * localAlphaHats))\n",
    "    def loss(w):\n",
    "        lossVal = 0\n",
    "        for wi,aH in zip(w, globalAlphaHats):\n",
    "            den = 1 / np.sum(np.multiply(n,wi))\n",
    "            wiXA = np.multiply(wi,localAlphaHats)\n",
    "            dot = np.sum(np.multiply(wiXA,n))\n",
    "            tilde =  den * dot\n",
    "            lossVal = lossVal + .5 * np.square(aH - tilde)\n",
    "        lossVal = lossVal + regLambda * np.linalg.norm(w)\n",
    "        return lossVal\n",
    "    return loss\n",
    "\n",
    "def g1(dsi, n_epochs=100,eta=1,regLambda=1e-5):\n",
    "    NBags = dsi.numU.shape[0]\n",
    "    globalAlphaHats = dsi.globalAlphaHats\n",
    "    # initialize values for gradient method\n",
    "    a = dsi.alphaHats\n",
    "    a0 = dsi.alphaHats\n",
    "    n = np.tile(dsi.numU.reshape((-1,1)), (1,a.shape[1]))\n",
    "    w = np.random.uniform(low=0.01, high=1,size=(len(globalAlphaHats),n.shape[0],n.shape[1]))\n",
    "    maes = [np.mean(np.abs(a.mean(1) - dsi.trueAlphas.flatten()))]\n",
    "    for i in tqdm(range(n_epochs), total=n_epochs):\n",
    "        # A iteration\n",
    "        alphaLossFn = aL0(w,n,globalAlphaHats,a0)\n",
    "        alphaGrad = grad(alphaLossFn)\n",
    "        a = a - eta * alphaGrad(a)\n",
    "#         a = 1 / (1 + np.exp(-a))\n",
    "        a = np.maximum(np.zeros_like(a),np.minimum(a,np.ones_like(a)))\n",
    "        # W iteration\n",
    "        wLossFn = wL0(a,n,globalAlphaHats,regLambda=regLambda)\n",
    "        wGrad = grad(wLossFn)\n",
    "        w = w - eta * wGrad(w)\n",
    "        maes.append(np.mean(np.abs(a.mean(1) - dsi.trueAlphas.flatten())))\n",
    "    return {\"maes\":maes,\n",
    "            \"alphaHats\": a,\n",
    "            \"weights\": w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g2(dsi, n_epochs=100,eta=1,regLambda=1e-5):\n",
    "    NBags = dsi.numU.shape[0]\n",
    "    globalAlphaHats = dsi.globalAlphaHats\n",
    "    # initialize values for gradient method\n",
    "    a = dsi.alphaHats\n",
    "    n = np.tile(dsi.numU.reshape((-1,1)), (1,a.shape[1])).flatten()\n",
    "    w = np.random.uniform(low=0.01, high=1,size=(len(globalAlphaHats),\n",
    "                                                 n.shape[0]))\n",
    "    maes = [np.mean(np.abs(a.mean(1) - dsi.trueAlphas.flatten()))]\n",
    "    for i in tqdm(range(n_epochs), total=n_epochs):\n",
    "        # A iteration\n",
    "        alphaLossFn = getAlphaLoss(w,n,globalAlphaHats)\n",
    "        alphaGrad = grad(alphaLossFn)\n",
    "        a = a - eta * alphaGrad(a.flatten()).reshape(a.shape)\n",
    "        a = np.maximum(np.zeros_like(a),np.minimum(a,np.ones_like(a)))\n",
    "        # W iteration\n",
    "        wLossFn = getWLoss(a.flatten(),n,globalAlphaHats,regLambda=regLambda)\n",
    "        wGrad = grad(wLossFn)\n",
    "        w = w - eta * wGrad(w)\n",
    "        maes.append(np.mean(np.abs(a.mean(1) - dsi.trueAlphas.flatten())))\n",
    "    return {\"maes\":maes,\n",
    "            \"alphaHats\": a,\n",
    "            \"weights\": w,\n",
    "            \"baseline_mae\": np.mean(np.abs(dsi.trueAlphas.flatten() - globalAlphaHats.mean()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yangDistributionDifference(posMean, negMean, cov, p=1):\n",
    "        \"\"\"\n",
    "        Eq. (7) from :\n",
    "\n",
    "        Yang, R., Jiang, Y., Mathews, S. et al.\n",
    "        Data Min Knowl Disc (2019) 33: 995.\n",
    "        https://doi.org/10.1007/s10618-019-00622-6\n",
    "        \"\"\"\n",
    "        sampleSize = 1000\n",
    "        #negSample = np.random.beta(aNeg, bNeg, sampleSize)\n",
    "        #posSample = np.random.beta(aPos, bPos, sampleSize)\n",
    "        #negPDF_neg = ss.beta.pdf(negSample,aNeg,bNeg)\n",
    "        #posPDF_neg = ss.beta.pdf(negSample,aPos,bPos)\n",
    "        #negPDF_pos = ss.beta.pdf(posSample,aNeg,bNeg)\n",
    "        #posPDF_pos = ss.beta.pdf(posSample,aPos,bPos)\n",
    "        posSample = np.random.multivariate_normal(mean=posMean, cov=cov,size=sampleSize)\n",
    "        negSample = np.random.multivariate_normal(mean=negMean, cov=cov,size=sampleSize)\n",
    "        negPDF_neg = ss.multivariate_normal.pdf(negSample,mean=negMean, cov=cov)\n",
    "        posPDF_neg = ss.multivariate_normal.pdf(negSample,mean=posMean,cov=cov)\n",
    "        negPDF_pos = ss.multivariate_normal.pdf(posSample,mean=negMean,cov=cov)\n",
    "        posPDF_pos = ss.multivariate_normal.pdf(posSample,mean=posMean,cov=cov)\n",
    "        z = np.zeros(sampleSize)\n",
    "        pdfDiffPos_NEG, pdfDiffNeg_NEG, pdfMax_NEG = _yangHelper(negPDF_neg, posPDF_neg, z)\n",
    "        pdfDiffPos_POS, pdfDiffNeg_POS, pdfMax_POS = _yangHelper(negPDF_pos, posPDF_pos, z)\n",
    "        return _yH2(pdfDiffNeg_NEG, negPDF_neg, pdfDiffPos_POS, posPDF_pos, posPDF_neg, negPDF_pos, pdfMax_NEG, pdfMax_POS,p,sampleSize)\n",
    "\n",
    "def _yangHelper(negPDF,posPDF,z):\n",
    "        pdfDiff = negPDF - posPDF\n",
    "        pdfDiffNeg = np.maximum(pdfDiff, z)\n",
    "        minus1 = -1 * pdfDiff\n",
    "        pdfDiffPos = np.maximum(minus1, z)\n",
    "        pdfMax = np.maximum(negPDF, posPDF)\n",
    "        return pdfDiffPos, pdfDiffNeg, pdfMax\n",
    "\n",
    "def _yH2(pdfDiffNeg_NEG, negPDF_NEG, pdfDiffPos_POS, posPDF_POS, posPDF_NEG, negPDF_POS, pdfMax_NEG, pdfMax_POS,p,sampleSize):\n",
    "        numerator1 = np.mean(pdfDiffNeg_NEG / negPDF_NEG)\n",
    "        numerator2 = np.mean(pdfDiffPos_POS / posPDF_POS)\n",
    "        sumVecs = np.power(numerator1, np.ones_like(numerator1) * p) + np.power(numerator2, np.ones_like(numerator2) * p)\n",
    "        dPHat = np.power(sumVecs, np.ones_like(sumVecs) * (1/p))\n",
    "        dTermNeg = (posPDF_NEG * 0.5) + (negPDF_NEG * 0.5)\n",
    "        dTermPos = (posPDF_POS * 0.5) + (negPDF_POS * 0.5)\n",
    "        denominator = (np.sum(pdfMax_NEG / dTermNeg) + np.sum(pdfMax_POS / dTermPos)) / (2 * sampleSize)\n",
    "        return dPHat / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotResults(dsi,res):\n",
    "    # plot results\n",
    "    fig,ax = plt.subplots(1,3,figsize=(12,4))\n",
    "    # Plot MAEs\n",
    "    ax[0].plot(res[\"maes\"],label=\"gradient\")\n",
    "    # add global baseline\n",
    "    globalMAE = np.mean(np.abs(dsi.trueAlphas - dsi.globalAlphaHats.mean()))\n",
    "    ax[0].hlines(globalMAE, 0,len(res[\"maes\"]),color=\"black\",label=\"global\")\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(\"Gradient Method MAE\")\n",
    "    # Plot final alphaHat\n",
    "    N = len(dsi.numU)\n",
    "    K = len(dsi.globalAlphaHats)\n",
    "    for i in range(N):\n",
    "#         ax[1].fill_between(np.array([res[\"alphaHats\"][i].min(),\n",
    "#                                      res[\"alphaHats\"][i].max()]),\n",
    "#                            y1=1,\n",
    "#                            y2=dsi.numU[i]+.25,\n",
    "#                             alpha=.25,color=\"red\")\n",
    "        ax[1].vlines(res[\"alphaHats\"][i].mean(),0,dsi.numU[i]+.15,color=\"red\")\n",
    "    ax[1].vlines(dsi.globalAlphaHats.mean(),\n",
    "                 0,\n",
    "                 max(dsi.numU),\n",
    "                 color=\"black\",label=r\"$\\hat{\\alpha_{c_i}}$\")\n",
    "    ax[1].vlines(dsi.alphaHats.mean(1),\n",
    "                 0,\n",
    "                 dsi.numU-.15,\n",
    "                 color=\"blue\",label=r\"$\\hat{\\alpha}_0$\")\n",
    "    ax[1].vlines(dsi.trueAlphas,0,dsi.numU - .25,color=\"green\",label=r\"$\\alpha$\")\n",
    "    ax[1].set_title(\"Alphas\")\n",
    "#     ax[1].set_xlim(0,1)\n",
    "    ax[1].legend(loc=\"lower right\")\n",
    "    # plot weights\n",
    "    #ax[2].vlines(res[\"weights\"],0,np.tile(dsi.numU,(K,1)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae0,ae1,bae,lae = 0, 0, 0, 0\n",
    "N = 0\n",
    "for rep in tqdm(range(10),total=10,desc=\"reps\"):\n",
    "#     print(\"rep: {}\".format(rep))\n",
    "    # build dataset\n",
    "    n_epochs = 100\n",
    "    dsi = initDS(ds_size=1000,n_alpha_ests=10,alphaDistr=lambda: np.random.uniform(0.05,.95))\n",
    "    # Run gradient method\n",
    "    g1Results = g1(dsi,n_epochs=n_epochs)\n",
    "#     plotResults(dsi,g1Results)\n",
    "    g2Res = g2(dsi,n_epochs=n_epochs)\n",
    "#     plotResults(dsi,g2Res)\n",
    "    ae0 += g1Results[\"maes\"][-1] * dsi.N\n",
    "    ae1 += g2Res[\"maes\"][-1] * dsi.N\n",
    "    bae += g2Res[\"baseline_mae\"] * dsi.N\n",
    "    lae += g2Res[\"maes\"][0] * dsi.N\n",
    "    N += dsi.N\n",
    "print(\"gradient1: {}\\n gradient2: {}\\n global: {}\\n local: {}\".format(ae0/N, ae1/N, bae / N, lae/N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initRealDS(filename, size=100,n_alpha_ests=10):\n",
    "    dsi = getRealDS(fileName,size,\n",
    "                    nPDistr=lambda: np.random.choice(np.arange(1,5).astype(int)),\n",
    "                    nUDistr=lambda: np.random.choice(np.arange(20,30).astype(int)),\n",
    "                    alphaDistr=lambda: np.random.uniform(0.05,1))\n",
    "    dsi.alphaHats,dsi.curves = getBagAlphaHats(dsi,numbootstraps=n_alpha_ests)\n",
    "    dsi.globalAlphaHats = getAlphaHat(dsi,reps=n_alpha_ests)\n",
    "    return dsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ae0,ae1,bae,lae = 0, 0, 0, 0\n",
    "# N = 0\n",
    "# fileNames = glob(\"/home/dzeiberg/ClassPriorEstimation/rawDatasets/*.mat\")[1:]\n",
    "# np.random.shuffle(fileNames)\n",
    "# for fileName in tqdm(fileNames, total=len(fileNames),desc=\"reps\"):\n",
    "#     name = fileName.split(\"/\")[-1].replace(\".mat\",\"\")\n",
    "#     # build dataset\n",
    "#     size = 1000\n",
    "#     dsi = initRealDS(fileName,size=size,\n",
    "#                      n_alpha_ests=10)\n",
    "#     print(\"dataset: {}\".format(name))\n",
    "#     # build dataset\n",
    "#     n_epochs = 100\n",
    "#     # Run gradient method\n",
    "#     g1Results = g1(dsi,n_epochs=n_epochs)\n",
    "# #     plotResults(dsi,g1Results)\n",
    "#     g2Res = g2(dsi,n_epochs=n_epochs)\n",
    "# #     plotResults(dsi,g2Res)\n",
    "#     ae0 += g1Results[\"maes\"][-1] * dsi.N\n",
    "#     ae1 += g2Res[\"maes\"][-1] * dsi.N\n",
    "#     bae += g2Res[\"baseline_mae\"] * dsi.N\n",
    "#     lae += g2Res[\"maes\"][0] * dsi.N\n",
    "#     N += dsi.N\n",
    "# print(\"gradient1: {}\\n gradient2: {}\\n global: {}\\n local: {}\".format(ae0/N, ae1/N, bae / N, lae/N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

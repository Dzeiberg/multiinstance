{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp gradientMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Based Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\alpha_i}$: the local dictCurve estimate for the $i^{th}$ bag\n",
    "\n",
    "$\\hat{\\alpha_{c_i}}$: the $i^{th}$ global distCurve estimate using bootstrapped sample\n",
    "\n",
    "$w_{ji}$: the contribution of bag j to the $i^{th}$ global estimate\n",
    "\n",
    "$\\tilde{\\alpha_i}$: the expected global class prior given the current contribution values and local estimates for each bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{\\alpha_i} = \\frac{w_{1i} \\cdot \\hat{\\alpha_1} \\cdot n_1 \\dots w_{Ni} \\cdot \\hat{\\alpha_N} \\cdot n_N}{w_{1i} \\cdot n_1 \\dots w_{Ni} \\cdot n_N} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss for cluster $c_i$\n",
    "\n",
    "\n",
    "$\\mathcal{L}_{c_i} = \\frac{1}{2}(\\tilde{\\alpha_i} - \\hat{\\alpha_{c_i}})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def gradientMethod(ds):\n",
    "        alphaHat : init alphaHat for each bag\n",
    "        alpha_C : get K global alpha estimates\n",
    "        init W randomly\n",
    "        for each iteration:\n",
    "            # calcualte loss given the current values of alphaHat and w\n",
    "            loss = lossFunction(w[:,1], alpha_C[1]) + ... + lossFunction(w[:,K], alpha_C[K])\n",
    "            # update alphaHat\n",
    "            alphaHat = alphaHat - eta * grad(loss)\n",
    "            # calculate the loss give the current w and new alphaHats\n",
    "            loss = lossFunction(1) + ... + lossFunction(K)\n",
    "            w = w - eta * grad(loss)\n",
    "            getMAE(alphaHat, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from multiinstance.dataset_utils import buildDataset\n",
    "from multiinstance.utils import *\n",
    "from multiinstance.distanceApproaches import *\n",
    "from multiinstance.agglomerative_clustering import AgglomerativeClustering\n",
    "from numba import set_num_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_num_threads(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "    \n",
    "def getAlphaLoss(w,n, alphaHats):\n",
    "    def loss(alpha):\n",
    "        lossVal = 0\n",
    "        for wi, aH in zip(w, alphaHats):\n",
    "            tilde = (1 / np.dot(wi,n)) * np.dot(np.multiply(alpha,wi),n)\n",
    "            lossVal += .5 * np.square(aH - tilde)\n",
    "        return lossVal\n",
    "    return loss\n",
    "    \n",
    "def getWLoss(a,n, alphaHats):\n",
    "    def loss(w):\n",
    "        lossVal = 0\n",
    "        for wi,aH in zip(w, alphaHats):\n",
    "            tilde = (1 / np.dot(wi,n)) * np.dot(np.multiply(a,wi),n)\n",
    "            lossVal += .5 * np.square(aH - tilde)\n",
    "        return lossVal\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def getAlphaHat(dsi,reps=10):\n",
    "    P, U = list(zip(*[dsi.getBag(int(i)) for i in range(dsi.N)]))\n",
    "    p = np.concatenate(P)\n",
    "    u = np.concatenate(U)\n",
    "    alphaHats,_ = getEsts(p,u,reps)\n",
    "    return alphaHats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def gradientMethod(dsi, n_epochs=100):\n",
    "    alphaHats = dsi.globalAlphaHats\n",
    "    # initialize values for gradient method\n",
    "    a = dsi.alphaHats.mean(1)\n",
    "    n = dsi.numU\n",
    "    w = np.random.uniform(low=0.01,high=1,size=(len(alphaHats),\n",
    "                                                n.shape[0]))\n",
    "    maes = [np.mean(np.abs(a - dsi.trueAlphas.flatten()))]\n",
    "    # Run gradient method\n",
    "    for i in tqdm(range(n_epochs),total=n_epochs):\n",
    "        alphaLossFn = getAlphaLoss(w,n,alphaHats)\n",
    "        alphaGrad = grad(alphaLossFn)\n",
    "        a = a - .025 * alphaGrad(a)\n",
    "        wLossFn = getWLoss(a,n,alphaHats)\n",
    "        wGrad = grad(wLossFn)\n",
    "        w = w - .025 * wGrad(w)\n",
    "        maes.append(np.mean(np.abs(a - dsi.trueAlphas.flatten())))\n",
    "    return maes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g2(dsi, n_epochs=100):\n",
    "    alphaHats = dsi.globalAlphaHats\n",
    "    # initialize values for gradient method\n",
    "    a = dsi.alphaHats\n",
    "    n = np.tile(dsi.numU.reshape((-1,1)), (1,a.shape[1])).flatten()\n",
    "    w = np.random.uniform(low=0.01, high=1,size=(len(alphaHats),\n",
    "                                                 n.shape[0]))\n",
    "    maes = [np.mean(np.abs(a.mean(1) - dsi.trueAlphas.flatten()))]\n",
    "    for i in tqdm(range(n_epochs), total=n_epochs):\n",
    "        alphaLossFn = getAlphaLoss(w,n,alphaHats)\n",
    "        alphaGrad = grad(alphaLossFn)\n",
    "        a = a - alphaGrad(a.flatten()).reshape(a.shape)\n",
    "        wLossFn = getWLoss(a.flatten(),n,alphaHats)\n",
    "        wGrad = grad(wLossFn)\n",
    "        w = w - .025 * wGrad(w)\n",
    "        maes.append(np.mean(np.abs(a.mean(1) - dsi.trueAlphas.flatten())))\n",
    "    return maes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ss.beta(500,500).pdf(np.arange(0,1,.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bimodal():\n",
    "    if np.random.binomial(1,.5):\n",
    "        return np.random.beta(2,10)\n",
    "    return np.random.beta(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initDS(ds_size=100,n_alpha_ests=50, nP=None, nU=None,\n",
    "           alphaDistr=lambda: np.random.uniform(0.1,.5),posMean=None, negMean=None,cov=None):\n",
    "    dsi = buildDataset(ds_size,alphaDistr=alphaDistr, nP=nP,\n",
    "                       nU=nU,posMean=posMean, negMean=negMean,cov=cov)\n",
    "\n",
    "#     dsi = addTransformScores(dsi)\n",
    "    dsi.alphaHats,dsi.curves = getBagAlphaHats(dsi,numbootstraps=n_alpha_ests)\n",
    "    dsi.globalAlphaHats = getAlphaHat(dsi,reps=n_alpha_ests)\n",
    "    return dsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yangDistributionDifference(posMean, negMean, cov, p=1):\n",
    "        \"\"\"\n",
    "        Eq. (7) from :\n",
    "\n",
    "        Yang, R., Jiang, Y., Mathews, S. et al.\n",
    "        Data Min Knowl Disc (2019) 33: 995.\n",
    "        https://doi.org/10.1007/s10618-019-00622-6\n",
    "        \"\"\"\n",
    "        sampleSize = 1000\n",
    "        #negSample = np.random.beta(aNeg, bNeg, sampleSize)\n",
    "        #posSample = np.random.beta(aPos, bPos, sampleSize)\n",
    "        #negPDF_neg = ss.beta.pdf(negSample,aNeg,bNeg)\n",
    "        #posPDF_neg = ss.beta.pdf(negSample,aPos,bPos)\n",
    "        #negPDF_pos = ss.beta.pdf(posSample,aNeg,bNeg)\n",
    "        #posPDF_pos = ss.beta.pdf(posSample,aPos,bPos)\n",
    "        posSample = np.random.multivariate_normal(mean=posMean, cov=cov,size=sampleSize)\n",
    "        negSample = np.random.multivariate_normal(mean=negMean, cov=cov,size=sampleSize)\n",
    "        negPDF_neg = ss.multivariate_normal.pdf(negSample,mean=negMean, cov=cov)\n",
    "        posPDF_neg = ss.multivariate_normal.pdf(negSample,mean=posMean,cov=cov)\n",
    "        negPDF_pos = ss.multivariate_normal.pdf(posSample,mean=negMean,cov=cov)\n",
    "        posPDF_pos = ss.multivariate_normal.pdf(posSample,mean=posMean,cov=cov)\n",
    "        z = np.zeros(sampleSize)\n",
    "        pdfDiffPos_NEG, pdfDiffNeg_NEG, pdfMax_NEG = _yangHelper(negPDF_neg, posPDF_neg, z)\n",
    "        pdfDiffPos_POS, pdfDiffNeg_POS, pdfMax_POS = _yangHelper(negPDF_pos, posPDF_pos, z)\n",
    "        return _yH2(pdfDiffNeg_NEG, negPDF_neg, pdfDiffPos_POS, posPDF_pos, posPDF_neg, negPDF_pos, pdfMax_NEG, pdfMax_POS,p,sampleSize)\n",
    "\n",
    "def _yangHelper(negPDF,posPDF,z):\n",
    "        pdfDiff = negPDF - posPDF\n",
    "        pdfDiffNeg = np.maximum(pdfDiff, z)\n",
    "        minus1 = -1 * pdfDiff\n",
    "        pdfDiffPos = np.maximum(minus1, z)\n",
    "        pdfMax = np.maximum(negPDF, posPDF)\n",
    "        return pdfDiffPos, pdfDiffNeg, pdfMax\n",
    "\n",
    "def _yH2(pdfDiffNeg_NEG, negPDF_NEG, pdfDiffPos_POS, posPDF_POS, posPDF_NEG, negPDF_POS, pdfMax_NEG, pdfMax_POS,p,sampleSize):\n",
    "        numerator1 = np.mean(pdfDiffNeg_NEG / negPDF_NEG)\n",
    "        numerator2 = np.mean(pdfDiffPos_POS / posPDF_POS)\n",
    "        sumVecs = np.power(numerator1, np.ones_like(numerator1) * p) + np.power(numerator2, np.ones_like(numerator2) * p)\n",
    "        dPHat = np.power(sumVecs, np.ones_like(sumVecs) * (1/p))\n",
    "        dTermNeg = (posPDF_NEG * 0.5) + (negPDF_NEG * 0.5)\n",
    "        dTermPos = (posPDF_POS * 0.5) + (negPDF_POS * 0.5)\n",
    "        denominator = (np.sum(pdfMax_NEG / dTermNeg) + np.sum(pdfMax_POS / dTermPos)) / (2 * sampleSize)\n",
    "        return dPHat / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yangDistributionDifference(np.array([1,2]),np.array([3,2]),np.eye(2),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rep in tqdm(range(10),total=10,desc=\"reps\"):\n",
    "    # build dataset\n",
    "    dsi = initDS(ds_size=25, n_alpha_ests=10)\n",
    "    # Run gradient method\n",
    "    maes = g2(dsi,n_epochs=100)\n",
    "    # Run agglomerative clustering\n",
    "    agg0 = AgglomerativeClustering(dsi, .5,use_alphas_as_scores=True)\n",
    "    agg0.cluster()\n",
    "    # plot results\n",
    "    fig,ax = plt.subplots(1,5,figsize=(20,4))\n",
    "    # Plot MAEs\n",
    "    ax[0].plot(maes,label=\"gradient\")\n",
    "    maes2 =agg0.meanAbsErrs\n",
    "    ax[0].plot(maes2, label=\"agg\")\n",
    "    globalMAE = np.mean(np.abs(dsi.trueAlphas - dsi.globalAlphaHats.mean()))\n",
    "    ax[0].hlines(globalMAE, 0,100)\n",
    "    ax[0].legend()\n",
    "    ax[1].hist(dsi.trueAlphas)\n",
    "    ax[1].set_title(r\"$\\alpha$\")\n",
    "    ax[2].hist(dsi.numP)\n",
    "    ax[2].set_title(\"Num Positive\")\n",
    "    ax[3].hist(dsi.numU)\n",
    "    ax[3].set_title(\"Num Unlabeled\")\n",
    "    ax[4].hist([h[:n].sum() for h,n in zip(dsi.hiddenLabels, dsi.numU)])\n",
    "    ax[4].set_title(\"Num Unlabeled Positive\")\n",
    "    fig.suptitle(\"Distr Distance: {:.4f}    dim:{}\".format(yangDistributionDifference(dsi.posDistMean,dsi.negDistMean,dsi.cov),\n",
    "                                                          dsi.posDistMean.shape))\n",
    "    plt.savefig(\"figs/nb_09/distrDistFigs/fig_{}.pdf\".format(rep),format=\"pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosis\n",
    "\n",
    "After all, I wasn't using the same distribution for each data set but was sampling the dimension and the means of the mvn distributions randomly. The method performs well on the data sets in which the distributions are further away, but will lead to MAE values worse than that of the local estimates when the distributions have smaller distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

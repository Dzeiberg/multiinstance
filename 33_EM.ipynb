{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as ss\n",
    "from easydict import EasyDict\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.metrics import roc_auc_score,adjusted_mutual_info_score, adjusted_rand_score\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bag$_i$**\n",
    "\n",
    "\n",
    "**latent positive class conditional distribution**\n",
    "* $z_k = 1$ indicates instance came from component k\n",
    "\n",
    "$p^i(z_k = 1 | y=1) = \\pi^i_k$\n",
    "\n",
    "**latent negative class conditional distribution**\n",
    "\n",
    "$p^i(z_k=1 | y=0) = \\rho^i_k$\n",
    "\n",
    "**latent positive posterior distribution**\n",
    "\n",
    "$ p(z_k=1|x, y=1) = \\gamma(z_k) = \\frac{\\pi^i_k \\phi^1_k(x;\\mu^1_k,\\Sigma^1_k)}{\\sum_{j=1}^{K_1} \\pi^i_j \\phi^1_j(x;\\mu^1_j,\\Sigma^1_j)} $\n",
    "\n",
    "**Latent Posterior**\n",
    "\n",
    "I don't think this is right, should be p(y=1|x) not p(y=1)\n",
    "\n",
    "$ p(z_k=1|x) = p(y=1) p(z_k=1|y=1,x) + p(y=0)p(z_k=1|y=0,x) = \\alpha \\frac{\\pi_k^i \\phi^1_k(x)}{\\sum_{j=1}^{K_1} \\pi_k^j \\phi^1_j(x)} + (1-\\alpha)\\frac{\\rho_k^i \\phi^0_k(x)}{\\sum_{j=1}^{K_0}\\rho_j^i \\phi^0_j(x)}$\n",
    "\n",
    "**positive class conditional distribution**\n",
    "* K-component gaussian mixture model\n",
    "* components shared across bags\n",
    "* mixture weights unique to each bag\n",
    "\n",
    "$p^i(x|y=1) = \\sum_{k=1}^{K_1} \\pi^i_k \\phi^1_k(x;\\mu^1_k,\\Sigma^1_k)$\n",
    "\n",
    "**marginal distribution**\n",
    "\n",
    "$p(x|b_i) = p(y=1|b_i)p(x|y=1,b_i) + p(y=0|b_i)p(x|y=0,b_i)$\n",
    "\n",
    "$p^i(x) =\\alpha_i \\sum_{k=1}^{K_1} \\pi^i_k \\phi^1_k(x;\\mu^1_k,\\Sigma^1_k) + (1-\\alpha_i) \\sum_{k=1}^{K_0}\\rho^i_k \\phi^0_k(x;\\mu^0_k;\\Sigma^0_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag$_i \\cup$ Bag$_j$\n",
    "\n",
    "$p(x)= w_i [(\\alpha_i \\sum_{k=1}^{K_1} \\pi^i_k \\phi^1_k(x;\\mu^1_k,\\Sigma^1_k) + (1-\\alpha_i) \\sum_{k=1}^{K_0}\\rho^i_k \\phi^0_k(x;\\mu^0_k;\\Sigma^0_k))] + w_j[\\alpha_j \\sum_{k=1}^{K_1} \\pi^j_k \\phi^1_k(x;\\mu^1_k,\\Sigma^1_k) + (1-\\alpha_j) \\sum_{k=1}^{K_0}\\rho^j_k \\phi^0_k(x;\\mu^0_k;\\Sigma^0_k)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged Bags\n",
    "* $b_i := \\{c_i,m_i\\}$ : bag with component and mixture sample \n",
    "* B : set of bags {$b_i$}\n",
    "* positive bag weight : $w^1_i = \\frac{|c_i|}{\\sum_{j\\in B} |c_j|}$\n",
    "* unlabeled bag weight : $w_i = \\frac{|m_i|}{\\sum_{j\\in B} |m_j|}$\n",
    "\n",
    "**Positive Latent Marginal**\n",
    "\n",
    "$ p(z_k = 1|y=1) = \\sum_{i=1}^{|B|} p(b_i)p(z_k=1|b_i,y=1) = \\sum_{i=1}^{|B|} w^1_i \\pi_k^i = \\pi_k^B$\n",
    "\n",
    "**Positive conditional latent posterior**\n",
    "\n",
    "$p(z_k=1|x,y=1) = \\gamma^B(z_k) = \\frac{\\pi_k^B \\phi_k(x)}{\\sum_{j=1}^K \\pi_j^B \\phi_j(x))}$\n",
    "\n",
    "**Positive Class Conditional Distribution**\n",
    "\n",
    "$p(x|y=1) = \\sum_{k=1}^{K_1} p(z_k=1 | y=1)p(x|z_k=1,y=1) = \\sum_{k=1}^{K_1}(\\sum_{i=1}^{|B|} w^1_i \\pi^i_k) \\phi^1_k(x;\\mu^1_k,\\Sigma^1_k)$\n",
    "\n",
    "**Marginal Distribution**\n",
    "\n",
    "$p(x) = \\sum_{k=1}^{K_1} (\\sum_{i=1}^{|B|}w_i \\alpha_i \\pi_k^i) \\phi_k^1(x|\\mu^1_k,\\Sigma^1_k) + \\sum_{k=1}^{K_0}(\\sum_{i=1}^{|B|}w_i (1-\\alpha_i)\\rho_k^i)\\phi^0_k(x;\\mu^0_k,\\Sigma^0_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative Class Conditional**\n",
    "\n",
    "$p(x|y=0) = \\sum_{i=1}^{|B|} p(x,b_i|y=0)$\n",
    "\n",
    "$p(x|y=0) =\\sum_{i=1}^{|B|} p(x|b_i,y=0)p(b_i|y=0)$\n",
    "\n",
    "$p(x|y=0) = \\sum_{i=1}^{|B|} p(x|b_i,y=0)\\frac{p(y=0|b_i)p(b_i)}{p(y=0)}$\n",
    "\n",
    "$p(x|y=0) = \\sum_{i=1}^{|B|}(\\sum_{k=1}^{K} p(x,z_k=1|b_i,y=0))\\frac{p(y=0|b_i)p(b_i)}{p(y=0)}$\n",
    "\n",
    "$p(x|y=0) =  \\sum_{i=1}^{|B|}(\\sum_{k=1}^{K} p(x|z_k=1, b_i,y=0)p(z_k=1|b_i,y=0))\\frac{p(y=0|b_i)p(b_i)}{p(y=0)}$\n",
    "\n",
    "$p(x|y=0) = \\sum_{i=1}^{|B|}(\\sum_{k=1}^K \\phi_k^0(x)\\rho_k^i) \\frac{(1-\\alpha_i)w_i}{\\sum_{i=1}^{|B|}(1-\\alpha_i)w_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# export\n",
    "def KMeansInit(X, k):\n",
    "    means = [X[np.random.choice(np.arange(X.shape[0]))]]\n",
    "    for iteration in range(k-1):\n",
    "        dists = cdist(X, np.stack(means)).min(1)\n",
    "        probs = dists / dists.sum()\n",
    "        nextIndex = np.random.choice(np.arange(X.shape[0]),p=probs)\n",
    "        means.append(X[nextIndex])\n",
    "    return np.stack(means)\n",
    "\n",
    "class EM:\n",
    "    def __init__(self, X, k):\n",
    "        self.X = X\n",
    "        self.k = k\n",
    "        self.N,self.dim = self.X.shape\n",
    "        self.initializeComponents()\n",
    "        \n",
    "    def initializeComponents(self):\n",
    "        mins = self.X.min(0)\n",
    "        maxs = self.X.max(0)\n",
    "        # USE K-MEANS++\n",
    "#         self.mus = KMeansInit(self.X, self.k)\n",
    "        self.mus = self.X[np.random.choice(np.arange(self.X.shape[0]),size=self.k)]\n",
    "        self.covs = np.stack([np.eye(self.dim) for _ in range(self.k)])\n",
    "        self.pi = np.random.dirichlet(np.ones(self.k))\n",
    "        \n",
    "    def E_Step(self):\n",
    "        gammas = np.zeros((self.N,self.k))\n",
    "        for n in range(self.N):\n",
    "            for k in range(self.k):\n",
    "                gammas[n,k] = self.pi[k] * ss.multivariate_normal.pdf(self.X[n],\n",
    "                                                                      mean=self.mus[k],\n",
    "                                                                      cov=self.covs[k])\n",
    "            gammas[n] = gammas[n] / gammas[n].sum()\n",
    "        self.gammas = gammas\n",
    "        \n",
    "    def M_Step(self):\n",
    "        Nk = np.sum(self.gammas, axis=0)[:,np.newaxis]\n",
    "        self.mus = np.dot(self.gammas.T, self.X) / Nk\n",
    "        self.pi = np.mean(self.gammas,axis=0)\n",
    "        for k in range(self.k):\n",
    "            x = np.matrix(self.X - self.mus[k,:])\n",
    "            gamma_diag = np.matrix(np.diag(self.gammas[:,k]))\n",
    "            sigma_k = x.T * gamma_diag * x\n",
    "            self.covs[k,:,:] = sigma_k / Nk[k]\n",
    "            \n",
    "            \n",
    "    def log_likelihood(self):\n",
    "        ll = 0\n",
    "        for xn in self.X:\n",
    "            ll += np.log(np.sum([pik * ss.multivariate_normal.pdf(xn,\n",
    "                                                                  mean=muk,\n",
    "                                                                  cov=covk) for pik,muk,covk in zip(self.pi,\n",
    "                                                                                                    self.mus,\n",
    "                                                                                                    self.covs)]))\n",
    "        return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Description\n",
    "Here, I test whether a multi-instance clustering approach can lead to better estimates of the latent cluster assignment than separate clustering processes. While Shantanu and I thought we should compare the resulting mixing proportions, I think comparing the cluster predictions derived from the latent posterior more directly compares the values of interest. I use the [adjusted rand index](https://en.wikipedia.org/wiki/Rand_index) to compare the quality of the clusterings of the two methods.\n",
    "\n",
    "As seen in the above experiments, the average adjusted rand index across all bags is higher when using the multi-instance clustering approach. Further, the multi-instance method ensures the estimated distributions $\\phi_k$ are the same for each bag. I do not evaluate the quality of the resulting distributions and believe the quality of the gaussians degrades in high dimensions.\n",
    "\n",
    "**Notes**\n",
    "* I'm not directly comparing the quality of the latent posterior but rather the clustering estimates that those posteriors lead to\n",
    "* I'm treating this evaluation as an alternative to separately comparing the quality of the latent marginal (r$^i_j$ i.e. p(z = k) ) and the fitted distributions $\\phi_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MultiEM:\n",
    "    def __init__(self, bags, n_components):\n",
    "        self.em_instances = []\n",
    "        self.labels = []\n",
    "        self.bags = bags\n",
    "        # Initialize each EM instance\n",
    "        for bag in self.bags:\n",
    "            self.em_instances.append(EM(bag.X_pos, n_components))\n",
    "        # Override the initialization\n",
    "\n",
    "    def run_positive_em(self, n_iters, multi=True, KMeansPPInit=True):\n",
    "        if KMeansPPInit:\n",
    "            if multi:\n",
    "                for inst in range(len(self.em_instances)):\n",
    "                    self.em_instances[inst].mus = KMeansInit(np.concatenate([e.X for e in self.em_instances]),\n",
    "                                                      self.em_instances[0].k)\n",
    "            else:\n",
    "                self.em_instances[0].mus = KMeansInit(np.concatenate([e.X for e in self.em_instances]),\n",
    "                                                      self.em_instances[0].k)\n",
    "        self.logLikelihoods = np.zeros((len(self.em_instances),\n",
    "                                        n_iters+2))\n",
    "        for inst in range(len(self.em_instances)):\n",
    "            self.logLikelihoods[inst,0] = self.em_instances[inst].log_likelihood()\n",
    "        for iteration in tqdm(range(1, n_iters+1),total=n_iters,leave=False):\n",
    "            for inst in range(len(self.em_instances)):\n",
    "                self.em_instances[inst].E_Step()\n",
    "                self.em_instances[inst].M_Step()\n",
    "                if multi:\n",
    "                    self.em_instances[(inst+1) % len(self.em_instances)].mus = self.em_instances[inst].mus\n",
    "                    self.em_instances[(inst+1) % len(self.em_instances)].covs = self.em_instances[inst].covs\n",
    "                self.logLikelihoods[inst,iteration] = self.em_instances[inst].log_likelihood()\n",
    "        # After final iteration, pass mu and cov to all instances and update gammas through the E_Step\n",
    "        for j in range(len(self.em_instances)):\n",
    "            if multi:\n",
    "                self.em_instances[j].mus = self.em_instances[0].mus\n",
    "                self.em_instances[j].covs = self.em_instances[0].covs\n",
    "                # update gamma with new mus and covs values\n",
    "                self.em_instances[j].E_Step()\n",
    "            self.logLikelihoods[j, -1] = self.em_instances[j].log_likelihood()\n",
    "        self.compute_positive_clustering_performance()\n",
    "\n",
    "    def compute_positive_clustering_performance(self):\n",
    "        adjusted_rand_scores = []\n",
    "        for em,bag in zip(self.em_instances, self.bags):\n",
    "            adjusted_rand_scores.append(adjusted_rand_score(bag.positive_component_labels,\n",
    "                                                            np.argmax(em.gammas,axis=1)))\n",
    "        self.score = np.mean(adjusted_rand_scores)\n",
    "\n",
    "    def cluster_unlabeled(self):\n",
    "        for bagnum, (bag, em) in enumerate(zip(self.bags, self.em_instances)):\n",
    "            assignments = np.zeros(bag.x_unlabeled.shape[0]).astype(int)\n",
    "            for instnum,inst in enumerate(bag.x_unlabeled):\n",
    "                lls = np.zeros(em.k)\n",
    "                for i,(mu,cov) in enumerate(zip(em.mus, em.covs)):\n",
    "                    lls[i] = ss.multivariate_normal.logpdf(inst,mean=mu,cov=cov)\n",
    "                assignments[instnum] = np.argmax(lls)\n",
    "            self.bags[bagnum].unlabeled_assignments = assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def generateBags(NBags,\n",
    "                 NPos=200,\n",
    "                 NUnlabeled=1000,\n",
    "                 pos_means=[[2,2], [-2,-2], [2,-2]],\n",
    "                 neg_means=[[0,0]],\n",
    "                 pos_covs=[np.eye(2)]*3,\n",
    "                 neg_covs=[np.eye(2)]):\n",
    "    bags = []\n",
    "    K = len(pos_means)\n",
    "    D = len(pos_means[0])\n",
    "    for _ in tqdm(range(NBags)):\n",
    "        bag = EasyDict()\n",
    "        bag.alpha = np.random.uniform()\n",
    "        bag.pi = np.random.dirichlet(np.ones(K)*1.5)\n",
    "        bag.rho = np.random.dirichlet(np.ones(K)*1.5)\n",
    "        bag.gamma = bag.alpha * bag.pi + (1 - bag.alpha) * bag.rho\n",
    "        bag.eta = bag.alpha * bag.pi / bag.gamma\n",
    "        bag.posClusterAssignment = np.random.choice(np.arange(K), p=bag.pi.ravel(),size=NPos)\n",
    "        bag.unlabeledPosClusterAssignment = np.random.choice(np.arange(K),\n",
    "                                                         p=bag.pi.ravel(),\n",
    "                                                         size=np.round(NUnlabeled * bag.alpha).astype(int))\n",
    "        upLabels, upCounts = np.unique(bag.unlabeledPosClusterAssignment,return_counts=True)\n",
    "        bag.empiricalPi = np.zeros(K)\n",
    "        for label,count in zip(upLabels,upCounts):\n",
    "            bag.empiricalPi[label] = count / upCounts.sum()\n",
    "        bag.unlabeledNegClusterAssignment = np.random.choice(np.arange(K),\n",
    "                                                         p=bag.rho.ravel(),\n",
    "                                                         size=np.round(NUnlabeled * (1 - bag.alpha)).astype(int))\n",
    "        unLabels, unCounts = np.unique(bag.unlabeledNegClusterAssignment,return_counts=True)\n",
    "        bag.empiricalRho = np.zeros(K)\n",
    "        for label,count in zip(unLabels,unCounts):\n",
    "            bag.empiricalRho[label] = count / unCounts.sum()\n",
    "        bag.empiricalGamma = bag.alpha * bag.empiricalPi + (1-bag.alpha) * bag.empiricalRho\n",
    "        bag.empiricalEta = bag.alpha * bag.empiricalPi / bag.empiricalGamma\n",
    "        bag.X_pos = np.zeros((0,D))\n",
    "        bag.hiddenLabels = np.concatenate((np.ones_like(bag.unlabeledPosClusterAssignment),\n",
    "                                           np.zeros_like(bag.unlabeledNegClusterAssignment)))\n",
    "        bag.x_unlabeled = np.zeros((0,D))\n",
    "        # sample positive data\n",
    "        for ci in tqdm(bag.posClusterAssignment,leave=False):\n",
    "            xi = np.random.multivariate_normal(pos_means[ci], pos_covs[ci])[None]\n",
    "            bag.X_pos = np.concatenate((bag.X_pos,xi))\n",
    "        \n",
    "        # sample unlabeled pos data\n",
    "        for ci in tqdm(bag.unlabeledPosClusterAssignment,leave=False):\n",
    "            xi = np.random.multivariate_normal(pos_means[ci], pos_covs[ci])[None]\n",
    "            bag.x_unlabeled = np.concatenate((bag.x_unlabeled, xi))\n",
    "        # sample unlabeled neg data\n",
    "        for ci in tqdm(bag.unlabeledNegClusterAssignment,leave=False):\n",
    "            xi = np.random.multivariate_normal(neg_means[ci], neg_covs[ci])[None]\n",
    "            bag.x_unlabeled = np.concatenate((bag.x_unlabeled, xi))\n",
    "        bags.append(bag)\n",
    "    return bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a775642ff14b26bea08399a0ab5f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/566 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bags = generateBags(1,\n",
    "                    NPos=1000,\n",
    "                    NUnlabeled=1000,\n",
    "                    neg_means=[[0,0],[-1,-1],[-2,-2]],\n",
    "                    neg_covs = [np.eye(2)]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23422853, 0.51923463, 0.24653684])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bags[0].pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23087432, 0.53005464, 0.23907104])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bags[0].empiricalPi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27997281, 0.45604099, 0.2639862 ])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bags[0].empiricalGamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28781205, 0.44668068, 0.26550727])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bags[0].gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05781562, 0.12358583, 0.03358507])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bags[0].eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06773035, 0.150059  , 0.02100531])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bags[0].empiricalEta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bags = generateBags(100,NPos=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = MultiEM(bags,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem.run_positive_em(10, multi=True, KMeansPPInit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aes = []\n",
    "bce = []\n",
    "for em, bag in zip(mem.em_instances, mem.bags):\n",
    "    aes.append(np.abs(em.pi - bag.pi))\n",
    "    bce.append(np.sum(-1 * bag.pi * np.log(em.pi)))\n",
    "print(\"pi_k MAE: {:.3f}\\nBCE: {:.3f}\".format(np.mean(aes), np.mean(bce)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem2 = MultiEM(bags,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem2.run_positive_em(10, multi=False,KMeansPPInit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aes2 = []\n",
    "bce2= []\n",
    "for em, bag in zip(mem2.em_instances, mem2.bags):\n",
    "    aes2.append(np.abs(em.pi - bag.pi))\n",
    "    bce2.append(np.sum(-1 * bag.pi * np.log(em.pi)))\n",
    "print(\"pi_k MAE: {:.3f}\\nBCE: {:.3f}\".format(np.mean(aes2), np.mean(bce2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem2.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem3 = MultiEM(bags,3)\n",
    "mem3.run_positive_em(10,multi=True,KMeansPPInit=False)\n",
    "aes3 = []\n",
    "bce3= []\n",
    "for em, bag in zip(mem3.em_instances, mem3.bags):\n",
    "    aes3.append(np.abs(em.pi - bag.pi))\n",
    "    bce3.append(np.sum(-1 * bag.pi * np.log(em.pi)))\n",
    "print(\"pi_k MAE: {:.3f}\\nBCE: {:.3f}\".format(np.mean(aes3), np.mean(bce3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem3.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem4 = MultiEM(bags,3)\n",
    "mem4.run_positive_em(10,multi=False,KMeansPPInit=False)\n",
    "aes4 = []\n",
    "bce4= []\n",
    "for em, bag in zip(mem4.em_instances, mem4.bags):\n",
    "    aes4.append(np.abs(em.pi - bag.pi))\n",
    "    bce4.append(np.sum(-1 * bag.pi * np.log(em.pi)))\n",
    "print(\"pi_k MAE: {:.3f}\\nBCE: {:.3f}\".format(np.mean(aes4), np.mean(bce4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem4.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in mem.logLikelihoods:\n",
    "    plt.plot(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in mem2.logLikelihoods:\n",
    "    plt.plot(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,len(mem.bags[:10]),figsize=(4 * len(mem.bags[:10]),4),sharey=True, sharex=True)\n",
    "for ds_num in range(len(mem.bags[:10])):\n",
    "    for comp in range(mem.em_instances[ds_num].k):\n",
    "        lambda_, v = np.linalg.eig(mem.em_instances[ds_num].covs[comp])\n",
    "        lambda_ = np.sqrt(lambda_)\n",
    "\n",
    "        ell = matplotlib.patches.Ellipse(xy=mem.em_instances[ds_num].mus[comp],\n",
    "                          width=lambda_[0]*2*2, height=lambda_[1]*2*2,\n",
    "                          angle=np.rad2deg(np.arccos(v[0, 0])),alpha=.5)\n",
    "        ax[ds_num].add_artist(ell)\n",
    "        # separate\n",
    "        lambda_, v = np.linalg.eig(mem2.em_instances[ds_num].covs[comp])\n",
    "        lambda_ = np.sqrt(lambda_)\n",
    "\n",
    "        ell = matplotlib.patches.Ellipse(xy=mem2.em_instances[ds_num].mus[comp],\n",
    "                          width=lambda_[0]*2*2, height=lambda_[1]*2*2,\n",
    "                          angle=np.rad2deg(np.arccos(v[0, 0])),alpha=.25,color=\"red\")\n",
    "        ax[ds_num].add_artist(ell)\n",
    "        # done\n",
    "    ax[ds_num].scatter(mem.bags[ds_num].X_pos[:,0],\n",
    "                       mem.bags[ds_num].X_pos[:,1],alpha=.5,color=\"green\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutl-EM with KMeans++:\n",
    "* MAE: 0.009\n",
    "* BCE: 1.033\n",
    "* clustering score: 0.9199\n",
    "\n",
    "Separate-EM with KMeans++\n",
    "* MAE: 0.145\n",
    "* BCE: 1.232\n",
    "* clustering score: 0.71\n",
    "\n",
    "Multi-EM without KMeans++\n",
    "* MAE: 0.167\n",
    "* BCE: 1.241\n",
    "* clustering score: 0.9199\n",
    "\n",
    "Separate-EM without KMeans++:\n",
    "* MAE: 0.145\n",
    "* BCE: 1.228\n",
    "* clustering score: 0.6708 (edited) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from multiinstance.data.syntheticData import buildDataset\n",
    "from scipy.stats import ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import leidenalg\n",
    "from itertools import chain\n",
    "\n",
    "from dist_curve.curve_constructor import makeCurve, plotCurve\n",
    "from dist_curve.transforms import getOptimalTransform\n",
    "from dist_curve.model import getTrainedEstimator\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "if os.path.isdir(\"/ssdata/\"):\n",
    "    pth = \"/home/dz/research/ClassPriorEstimation/model.hdf5\"\n",
    "else:\n",
    "    pth = \"/data/dzeiberg/ClassPriorEstimation/model.hdf5\"\n",
    "estimator = getTrainedEstimator(pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getOptimalTransform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def addTransformScores(ds, model=lambda: LogisticRegression(max_iter=1000),n_estimators=100):\n",
    "    P,U = list(zip(*[ds.getBag(i) for i in range(len(ds.numP))]))\n",
    "\n",
    "    P = np.concatenate(P)\n",
    "    U = np.concatenate(U)\n",
    "\n",
    "    X = np.concatenate((P,U))\n",
    "    Y = np.concatenate((np.ones(P.shape[0]),\n",
    "                        np.zeros(U.shape[0])))\n",
    "\n",
    "#     clf = BaggingClassifier(n_jobs=-1,base_estimator=model(), n_estimators=n_estimators, max_samples=X.shape[0],\n",
    "#                             max_features=X.shape[1], bootstrap=True, bootstrap_features=False, oob_score=True).fit(X,Y)\n",
    "\n",
    "#     probP = clf.oob_decision_function_[:,1]\n",
    "\n",
    "#     ds.aucPU = roc_auc_score(Y, probP)\n",
    "    transform_scores, auc_pu = getOptimalTransform(X,Y)\n",
    "    ds.aucPU = auc_pu\n",
    "    Pprobs, Uprobs = splitIntoBags(transform_scores,ds.numP, ds.numU)\n",
    "    ds.Pprobs = Pprobs\n",
    "    ds.Uprobs = Uprobs\n",
    "    return ds\n",
    "\n",
    "def splitIntoBags(probs, numP, numU):\n",
    "    probsP, probsU = probs[:numP.sum()], probs[numP.sum():]\n",
    "    pUpperIndices = np.concatenate(([0],np.cumsum(numP)))\n",
    "    uUpperIndices = np.concatenate(([0],np.cumsum(numU)))\n",
    "    P = np.zeros((len(numP), numP.max()))\n",
    "    U = np.zeros((len(numU), numU.max()))\n",
    "    for b in range(len(numP)):\n",
    "        P[b,:numP[b]] = probsP[pUpperIndices[b]:pUpperIndices[b+1]]\n",
    "        U[b,:numU[b]] = probsU[uUpperIndices[b] : uUpperIndices[b+1]]\n",
    "    return P,U\n",
    "\n",
    "def getTransformScores(ds,i):\n",
    "    p = ds.Pprobs[i,:ds.numP[i]]\n",
    "    u = ds.Uprobs[i,:ds.numU[i]]\n",
    "    return p,u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import matlab.engine\n",
    "eng = matlab.engine.start_matlab()\n",
    "path = eng.addpath(\"/home/dzeiberg/alphamax//alphamax/\")\n",
    "\n",
    "def getBootstrapSample(p,u):\n",
    "    ps = np.random.choice(np.arange(p.shape[0]), size=len(p), replace=True)\n",
    "    ps = p[ps]\n",
    "    us = np.random.choice(np.arange(u.shape[0]), size=len(u), replace=True)\n",
    "    us = u[us]\n",
    "    return ps, us\n",
    "\n",
    "def estimate(ps,us, useAlphaMax=False):\n",
    "    if useAlphaMax:\n",
    "        est = eng.runAlphaMax(matlab.double(us.tolist()),\n",
    "                        matlab.double(ps.tolist()),\n",
    "                        'transform','none')\n",
    "        curve = np.zeros(100)\n",
    "    else:\n",
    "        curve = makeCurve(ps,us).reshape((1,-1))\n",
    "        assert curve.sum() > 0\n",
    "        curve /= curve.sum()\n",
    "        est = estimator(curve)\n",
    "    return est,curve\n",
    "\n",
    "def getEsts(p,u, numbootstraps=10, useAlphaMax=False):\n",
    "    curves = np.zeros((numbootstraps, 100))\n",
    "    alphaHats = np.zeros(numbootstraps)\n",
    "    for i in tqdm(range(numbootstraps),total=numbootstraps,\n",
    "                  desc=\"getting distCurve Estimates\",leave=False):\n",
    "        ps, us = getBootstrapSample(p,u)\n",
    "        alphaHats[i],curves[i] = estimate(ps,us,useAlphaMax=useAlphaMax)\n",
    "    return alphaHats, curves\n",
    "\n",
    "def getBagAlphaHats(ds, numbootstraps=100,useAlphaMax=False):\n",
    "    alphaHats =np.zeros((ds.N, numbootstraps))\n",
    "    curves =np.zeros((ds.N, numbootstraps, 100))\n",
    "#     ps, _ = list(zip(*[ds.getBag(int(i)) for i in range(ds.N)]))\n",
    "    ps,_ = list(zip(*[getTransformScores(ds,i) for i in range(ds.N)]))\n",
    "    p = np.concatenate(ps).reshape((-1,1))\n",
    "    for bagIdx in tqdm(range(ds.N), total=ds.N, desc=\"getting bag estimates\",leave=False):\n",
    "        _,u = getTransformScores(ds,bagIdx)\n",
    "        u = u.reshape((-1,1))\n",
    "        alphaHats[bagIdx], curves[bagIdx] = getEsts(p,u, numbootstraps,useAlphaMax=useAlphaMax)\n",
    "    return alphaHats, curves\n",
    "\n",
    "\n",
    "def getCliqueAlphaHats(ds, cliques, numbootstraps=10):\n",
    "    Nc = len(cliques)\n",
    "    alphaHats = np.zeros((Nc, numbootstraps))\n",
    "    curves = np.zeros((Nc, numbootstraps, 100))\n",
    "    for cnum, clique in tqdm(enumerate(cliques), total=Nc, desc=\"getting clique alpha ests\", leave=False):\n",
    "        _, us = list(zip(*[ds.getBag(int(i)) for i in clique]))\n",
    "        ps, _ = list(zip(*[ds.getBag(int(i)) for i in range(ds.N)]))\n",
    "        p = np.concatenate(ps)\n",
    "        u = np.concatenate(us)\n",
    "        alphaHats[cnum], curves[cnum] = getEsts(p,u, numbootstraps)\n",
    "    return alphaHats, curves\n",
    "\n",
    "def getAlphaPrime(cliques, cliqueEsts):\n",
    "    bagNums = sorted(set(chain.from_iterable(cliques)))\n",
    "    alphaPrime = np.zeros(len(bagNums))\n",
    "    for bn in bagNums:\n",
    "        inClique = [bn in c for c in cliques]\n",
    "        alphaPrime[bn] = cliqueEsts[inClique].mean()\n",
    "    return alphaPrime\n",
    "\n",
    "def addGlobalEsts(dsi,reps=10, useAlphaMax=False):\n",
    "    alphaHats = np.zeros(reps)\n",
    "    for rep in tqdm(range(reps),total=reps,desc=\"getting global estimates\",leave=False):\n",
    "        P, U = list(zip(*[getBootstrapSample(*getTransformScores(dsi,i)) for i in range(dsi.N)]))\n",
    "        p = np.concatenate(P).reshape((-1,1))\n",
    "        u = np.concatenate(U).reshape((-1,1))\n",
    "        alphaHats[rep],_ = estimate(p,u,useAlphaMax=useAlphaMax)\n",
    "    dsi.globalAlphaHats = alphaHats\n",
    "    return dsi\n",
    "\n",
    "def addBagAlphaHats(dsi,reps=10,useAlphaMax=False):\n",
    "    alphaHats,curves = getBagAlphaHats(dsi,numbootstraps=reps,useAlphaMax=useAlphaMax)\n",
    "    dsi.alphaHats = alphaHats\n",
    "    return dsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def getKSMatrixPMatrix(samples):\n",
    "    \"Get Kolmogrov-Smirnov adjacency matrix from lists of lists of samples for each bag\"\n",
    "    N = samples.shape[0]\n",
    "    pmat = np.zeros((N,N))\n",
    "    for bag0Idx in tqdm(range(N),total=N, desc=\"making kolmogorov-smirnov adj matrix\", leave=False):\n",
    "        for bag1Idx in range(bag0Idx+ 1,N):\n",
    "            stat,p = ks_2samp(samples[bag0Idx], samples[bag1Idx])\n",
    "            pmat[bag0Idx, bag1Idx] = p\n",
    "            pmat[bag1Idx, bag0Idx] = p\n",
    "    return pmat\n",
    "\n",
    "def getAllCliques(mat, cutoffval=0.05):\n",
    "    \"\"\"\n",
    "    given matrix of pairwise test p-values, \n",
    "    make adjacency matrix using specified\n",
    "    confidence level then find all cliques for each bag\n",
    "    \"\"\"\n",
    "    adj = mat > cutoffval\n",
    "    g = nx.from_numpy_array(adj)\n",
    "    return list(nx.algorithms.clique.find_cliques(g))\n",
    "\n",
    "def clusterByLeidenAlg(similarityMatrix, resolution_parameter = 1.5):\n",
    "    \"\"\"\n",
    "    https://medium.com/@ciortanmadalina\n",
    "    This method partitions input data by applying the Leiden algorithm\n",
    "    on a given distance matrix.\n",
    "    \"\"\"\n",
    "    # convert distance matrix to similariy matrix\n",
    "    distanceMatrix = similarityMatrix\n",
    "    edges = np.unravel_index(np.arange(distanceMatrix.shape[0]*distanceMatrix.shape[1]), distanceMatrix.shape)\n",
    "    edges = list(zip(*edges))\n",
    "    weights = distanceMatrix.ravel()\n",
    "    \n",
    "    g = ig.Graph(directed=False)\n",
    "    g.add_vertices(distanceMatrix.shape[0])  # each observation is a node\n",
    "    g.add_edges(edges)\n",
    "    \n",
    "    g.es['weight'] = weights\n",
    "    weights = np.array(g.es[\"weight\"]).astype(np.float64)\n",
    "    partition_type = leidenalg.RBConfigurationVertexPartition\n",
    "    partition_kwargs = {}\n",
    "    partition_kwargs[\"weights\"] = weights\n",
    "    partition_kwargs[\"resolution_parameter\"] = resolution_parameter\n",
    "    part = leidenalg.find_partition(g, partition_type, **partition_kwargs)\n",
    "    groupAssignment = np.array(part.membership)\n",
    "    groups = [np.where(groupAssignment==g)[0] for g in np.unique(groupAssignment)]\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def getOptimalAdjacency(trueAlphas):\n",
    "    N = trueAlphas.shape[0]\n",
    "    adj = np.zeros((N,N))\n",
    "    for i,a0 in enumerate(trueAlphas):\n",
    "        for j,a1 in enumerate(trueAlphas[i+1:],start=i+1):\n",
    "            adj[i,j] = np.abs(a0 - a1)\n",
    "            adj[j,i] = np.abs(a0 - a1)\n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primeAEs = []\n",
    "baselineAEs = []\n",
    "localAEs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nreps = 10\n",
    "for rep in tqdm(range(Nreps),desc=\"dataset repetition\",total=Nreps):\n",
    "    ds0 = buildDataset(100, alphaDistr=lambda: np.random.uniform(0.01,0.5))\n",
    "\n",
    "    ds0.alphaHats, ds0.curves = getBagAlphaHats(ds0,10)\n",
    "\n",
    "    ds0.pmat = getKSMatrixPMatrix(ds0.alphaHats)\n",
    "    ds0.clusters = clusterByLeidenAlg(ds0.pmat > .05)\n",
    "#     ds0.clusters = getAllCliques(ds0.pmat, cutoffval=1e-5)\n",
    "#     ds0.adj = getOptimalAdjacency(ds0.trueAlphas)\n",
    "#     ds0.clusters = getAllCliques(ds0.adj)\n",
    "#     ds0.clusters = clusterByLeidenAlg(ds0.adj)\n",
    "    ds0.clusterAlphaHat, ds0.clusterCurves = getCliqueAlphaHats(ds0,ds0.clusters, numbootstraps=10)\n",
    "\n",
    "    ds0.alphaPrime = getAlphaPrime(ds0.clusters, ds0.clusterAlphaHat)\n",
    "\n",
    "    globalAlphaHat,_ = getCliqueAlphaHats(ds0, [np.arange(ds0.N)], numbootstraps=10)\n",
    "\n",
    "    primeAEs.append(np.abs(ds0.alphaPrime - ds0.trueAlphas))\n",
    "    localAEs.append(np.abs(ds0.alphaHats - ds0.trueAlphas))\n",
    "    baselineAEs.append(np.abs(ds0.trueAlphas - globalAlphaHat.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"clustering: {}  global: {}  local:{}\".format(np.mean(primeAEs), np.mean(baselineAEs), np.mean(localAEs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(v):\n",
    "    mi = v.min()\n",
    "    ma = v.max()\n",
    "    return (v - mi) / (ma - mi)\n",
    "\n",
    "def makeFig(curves, trueAlphas, localAlphaHat, globalAlpha, alphaPrimes,N=40):\n",
    "    ncols = 4\n",
    "#     N = curves.shape[0]\n",
    "    nrows = int(np.ceil(N/ncols))\n",
    "    fig,ax = plt.subplots(nrows,ncols,figsize=(20,20))\n",
    "    for bagNum in range(N):\n",
    "        axrow, axcol = int(bagNum / ncols), int(bagNum % ncols)\n",
    "        # shade between bin and max for each point in curve\n",
    "        curveQuants = np.quantile(minmax(curves[bagNum]),[0,1],axis=0)\n",
    "        ax[axrow,axcol].fill_between(np.arange(0,1,.01),\n",
    "                                     curveQuants[0],\n",
    "                                     y2=curveQuants[1],\n",
    "                                     color=\"blue\",alpha=0.25)\n",
    "        # Plot average curve line\n",
    "        ax[axrow,axcol].plot(np.arange(0,1,.01),\n",
    "                             minmax(curves[bagNum]).mean(0),\n",
    "                             color=\"magenta\")\n",
    "        # plot true alpha in black\n",
    "        ax[axrow,axcol].vlines(trueAlphas[bagNum],0,1,\n",
    "                               color=\"black\", alpha=.25)\n",
    "        # shade in range of alphas in red\n",
    "        ax[axrow,axcol].fill_betweenx([0,1],\n",
    "                          np.min(localAlphaHat[bagNum]),\n",
    "                          x2=np.max(localAlphaHat[bagNum]),\n",
    "                                      label=r\"$ \\hat{ \\alpha } $\"+ \" range\",\n",
    "                                      color=\"red\",alpha=.25)\n",
    "        # plot average local estimate in red\n",
    "        ax[axrow,axcol].vlines(np.mean(localAlphaHat[bagNum]),0,1,color=\"red\")\n",
    "        # plot adjusted estimate in green\n",
    "        ax[axrow,axcol].vlines(alphaPrimes[bagNum],0,1, label=\"alpha prime\",color=\"green\")\n",
    "        # Global Alpha in Blue\n",
    "        ax[axrow,axcol].vlines(globalAlpha, 0, 1, color=\"blue\")\n",
    "    plt.savefig(\"/ssdata/downloads/fig.pdf\",format=\"pdf\")\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterFig = makeFig(ds0.clusterCurves,\n",
    "                     [[ds0.trueAlphas[i] for i in c] for c in ds0.clusters],\n",
    "                     ds0.clusterAlphaHat,\n",
    "                     globalAlphaHat.mean(),\n",
    "                     np.zeros(ds0.clusterCurves.shape[0]),\n",
    "                     N=ds0.clusterCurves.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagfig = makeFig(ds0.curves, ds0.trueAlphas, ds0.alphaHats, np.mean(globalAlphaHat), ds0.alphaPrime,N=ds0.N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

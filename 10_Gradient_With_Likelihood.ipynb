{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp gradientMethod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Based Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{\\alpha_i}$: the local dictCurve estimate for the $i^{th}$ bag\n",
    "\n",
    "$\\hat{\\alpha_{c_i}}$: the $i^{th}$ global distCurve estimate using bootstrapped sample\n",
    "\n",
    "$w_{ji}$: the contribution of bag j to the $i^{th}$ global estimate\n",
    "\n",
    "$\\tilde{\\alpha_i}$: the expected global class prior given the current contribution values and local estimates for each bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{\\alpha_i} = \\frac{w_{1i} \\cdot \\hat{\\alpha_1} \\cdot n_1 \\dots w_{Ni} \\cdot \\hat{\\alpha_N} \\cdot n_N}{w_{1i} \\cdot n_1 \\dots w_{Ni} \\cdot n_N} $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss for cluster $c_i$\n",
    "\n",
    "\n",
    "$\\mathcal{L}_{c_i} = \\frac{1}{2}(\\tilde{\\alpha_i} - \\hat{\\alpha_{c_i}})^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def gradientMethod(ds):\n",
    "        alphaHat : init alphaHat for each bag\n",
    "        alpha_C : get K global alpha estimates\n",
    "        init W randomly\n",
    "        for each iteration:\n",
    "            # calcualte loss given the current values of alphaHat and w\n",
    "            loss = lossFunction(w[:,1], alpha_C[1]) + ... + lossFunction(w[:,K], alpha_C[K])\n",
    "            # update alphaHat\n",
    "            alphaHat = alphaHat - eta * grad(loss)\n",
    "            # calculate the loss give the current w and new alphaHats\n",
    "            loss = lossFunction(1) + ... + lossFunction(K)\n",
    "            w = w - eta * grad(loss)\n",
    "            getMAE(alphaHat, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "import autograd.scipy.stats as agss\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from multiinstance.dataset_utils import buildDataset\n",
    "from multiinstance.utils import *\n",
    "from multiinstance.distanceApproaches import *\n",
    "from multiinstance.agglomerative_clustering import AgglomerativeClustering\n",
    "from numba import set_num_threads\n",
    "\n",
    "import scipy.stats as ss\n",
    "from multiinstance.data.realData import buildDataset as getRealDS\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_num_threads(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bimodal():\n",
    "    if np.random.binomial(1,.5):\n",
    "        return np.random.beta(2,10)\n",
    "    return np.random.beta(10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def getAlphaHat(dsi,reps=10):\n",
    "    P, U = list(zip(*[dsi.getBag(int(i)) for i in range(dsi.N)]))\n",
    "    p = np.concatenate(P)\n",
    "    u = np.concatenate(U)\n",
    "    alphaHats,_ = getEsts(p,u,reps)\n",
    "    return alphaHats\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initDS(ds_size=100,n_alpha_ests=50, nP=None, nU=None,\n",
    "           alphaDistr=lambda: np.random.uniform(0.1,.5),posMean=None, negMean=None,cov=None):\n",
    "    dsi = buildDataset(ds_size,alphaDistr=alphaDistr, nP=nP,\n",
    "                       nU=nU,posMean=posMean, negMean=negMean,cov=cov)\n",
    "\n",
    "#     dsi = addTransformScores(dsi)\n",
    "    dsi.alphaHats,dsi.curves = getBagAlphaHats(dsi,numbootstraps=n_alpha_ests)\n",
    "    dsi.globalAlphaHats = getAlphaHat(dsi,reps=n_alpha_ests)\n",
    "    return dsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def getAlphaLoss(w,n, alphaHats):\n",
    "    def loss(localAlphaHats):\n",
    "        lossVal = 0\n",
    "        for wi, aH in zip(w, alphaHats):\n",
    "            tilde = (1 / np.dot(wi,n)) * np.dot(np.multiply(localAlphaHats,wi),n)\n",
    "            lossVal = lossVal + .5 * np.square(aH - tilde)\n",
    "        return lossVal\n",
    "    return loss\n",
    "\n",
    "def getAlphaLossWithLL(w,n, alphaHats,aMLEVals, bMLEVals, locMLE, scaleMLE):\n",
    "    def loss(localAlphaHats):\n",
    "        lossVal = 0\n",
    "        for wi, aH in zip(w, alphaHats):\n",
    "            tilde = (1 / np.dot(wi,n)) * np.dot(np.multiply(localAlphaHats,wi),n)\n",
    "            lossVal = lossVal + .5 * np.square(aH - tilde)\n",
    "        # add negative log-likelihood for each local bag to loss\n",
    "        for aHats, aMLE, bMLE,loc, scale in zip(localAlphaHats, aMLEVals,\n",
    "                                     bMLEVals, locMLE, scaleMLE):\n",
    "            lossVal = lossVal - 1e-3 * agss.beta.pdf((aHats - loc)/scale, a=aMLE, b=bMLE).sum()\n",
    "        return lossVal\n",
    "    return loss\n",
    "    \n",
    "def getWLoss(a,n, alphaHats, regLambda=1e-5):\n",
    "    def loss(w):\n",
    "        lossVal = 0\n",
    "        for wi,aH in zip(w, alphaHats):\n",
    "            den = (1 / np.dot(wi,n))\n",
    "            aXw = np.multiply(a,wi)\n",
    "            dot = np.dot(aXw,n)\n",
    "            tilde =  den * dot\n",
    "            lossVal = lossVal + .5 * np.square(aH - tilde)\n",
    "        lossVal = lossVal + regLambda * np.linalg.norm(w)\n",
    "        return lossVal\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g2(dsi, n_epochs=100,eta=1,regLambda=1e-5):\n",
    "    NBags = dsi.numU.shape[0]\n",
    "    globalAlphaHats = dsi.globalAlphaHats\n",
    "    # initialize values for gradient method\n",
    "    a = dsi.alphaHats\n",
    "    n = np.tile(dsi.numU.reshape((-1,1)), (1,a.shape[1])).flatten()\n",
    "    w = np.random.uniform(low=0.01, high=1,size=(len(globalAlphaHats),\n",
    "                                                 n.shape[0]))\n",
    "    maes = [np.mean(np.abs(a.mean(1) - dsi.trueAlphas.flatten()))]\n",
    "    for i in tqdm(range(n_epochs), total=n_epochs):\n",
    "        # A iteration\n",
    "        alphaLossFn = getAlphaLoss(w,n,globalAlphaHats)\n",
    "        alphaGrad = grad(alphaLossFn)\n",
    "        a = a - eta * alphaGrad(a.flatten()).reshape(a.shape)\n",
    "        # W iteration\n",
    "        wLossFn = getWLoss(a.flatten(),n,globalAlphaHats,regLambda=regLambda)\n",
    "        wGrad = grad(wLossFn)\n",
    "        w = w - eta * wGrad(w)\n",
    "        maes.append(np.mean(np.abs(a.mean(1) - dsi.trueAlphas.flatten())))\n",
    "    return maes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g2LL(dsi, n_epochs=100,eta=1):\n",
    "    NBags = dsi.numU.shape[0]\n",
    "    globalAlphaHats = dsi.globalAlphaHats\n",
    "    # initialize values for gradient method\n",
    "    a = dsi.alphaHats\n",
    "    n = np.tile(dsi.numU.reshape((-1,1)), (1,a.shape[1])).flatten()\n",
    "    w = np.random.uniform(low=0.01, high=1,size=(len(globalAlphaHats),\n",
    "                                                 n.shape[0]))\n",
    "    maes = [np.mean(np.abs(a.mean(1) - dsi.trueAlphas.flatten()))]\n",
    "    for i in tqdm(range(n_epochs), total=n_epochs):\n",
    "        aMLEVals = np.zeros(NBags)\n",
    "        bMLEVals = np.zeros_like(aMLEVals)\n",
    "        locMLE =np.zeros_like(aMLEVals)\n",
    "        scaleMLE = np.zeros_like(aMLEVals)\n",
    "        for bagNum in range(NBags):\n",
    "            try:\n",
    "                assert ~np.any(np.isinf(a))\n",
    "                aMLEVals[bagNum],bMLEVals[bagNum],locMLE[bagNum],scaleMLE[bagNum] = ss.beta.fit(a[bagNum])\n",
    "            except:\n",
    "                print(a[bagNum])\n",
    "                raise\n",
    "        # A iteration\n",
    "        alphaLossFn = getAlphaLossWithLL(w,n,globalAlphaHats,aMLEVals, bMLEVals, locMLE, scaleMLE)\n",
    "        alphaGrad = grad(alphaLossFn)\n",
    "        agrad = alphaGrad(a.flatten()).reshape(a.shape)\n",
    "        a = a - eta * np.maximum(np.ones_like(agrad) * -1, np.minimum(np.ones(agrad.shape),agrad))\n",
    "#         print(a)\n",
    "        assert ~np.isinf(a).any() and ~np.isnan(a).any()\n",
    "        # W iteration\n",
    "        wLossFn = getWLoss(a.flatten(),n,globalAlphaHats)\n",
    "#         mu = np.tile(aMLEVals / (aMLEVals + bMLEVals), (1,a.shape[1]))\n",
    "#         wLossFn = getWLoss(mu.flatten(),n,globalAlphaHats)\n",
    "        wGrad = grad(wLossFn)\n",
    "        wgradval = wGrad(w)\n",
    "        wgradval = np.maximum(np.ones_like(wgradval) * -1, np.minimum(wgradval, np.ones_like(wgradval)))\n",
    "        w = w - eta * wgradval\n",
    "        maes.append(np.mean(np.abs(a.mean(1) - dsi.trueAlphas.flatten())))\n",
    "    return maes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yangDistributionDifference(posMean, negMean, cov, p=1):\n",
    "        \"\"\"\n",
    "        Eq. (7) from :\n",
    "\n",
    "        Yang, R., Jiang, Y., Mathews, S. et al.\n",
    "        Data Min Knowl Disc (2019) 33: 995.\n",
    "        https://doi.org/10.1007/s10618-019-00622-6\n",
    "        \"\"\"\n",
    "        sampleSize = 1000\n",
    "        #negSample = np.random.beta(aNeg, bNeg, sampleSize)\n",
    "        #posSample = np.random.beta(aPos, bPos, sampleSize)\n",
    "        #negPDF_neg = ss.beta.pdf(negSample,aNeg,bNeg)\n",
    "        #posPDF_neg = ss.beta.pdf(negSample,aPos,bPos)\n",
    "        #negPDF_pos = ss.beta.pdf(posSample,aNeg,bNeg)\n",
    "        #posPDF_pos = ss.beta.pdf(posSample,aPos,bPos)\n",
    "        posSample = np.random.multivariate_normal(mean=posMean, cov=cov,size=sampleSize)\n",
    "        negSample = np.random.multivariate_normal(mean=negMean, cov=cov,size=sampleSize)\n",
    "        negPDF_neg = ss.multivariate_normal.pdf(negSample,mean=negMean, cov=cov)\n",
    "        posPDF_neg = ss.multivariate_normal.pdf(negSample,mean=posMean,cov=cov)\n",
    "        negPDF_pos = ss.multivariate_normal.pdf(posSample,mean=negMean,cov=cov)\n",
    "        posPDF_pos = ss.multivariate_normal.pdf(posSample,mean=posMean,cov=cov)\n",
    "        z = np.zeros(sampleSize)\n",
    "        pdfDiffPos_NEG, pdfDiffNeg_NEG, pdfMax_NEG = _yangHelper(negPDF_neg, posPDF_neg, z)\n",
    "        pdfDiffPos_POS, pdfDiffNeg_POS, pdfMax_POS = _yangHelper(negPDF_pos, posPDF_pos, z)\n",
    "        return _yH2(pdfDiffNeg_NEG, negPDF_neg, pdfDiffPos_POS, posPDF_pos, posPDF_neg, negPDF_pos, pdfMax_NEG, pdfMax_POS,p,sampleSize)\n",
    "\n",
    "def _yangHelper(negPDF,posPDF,z):\n",
    "        pdfDiff = negPDF - posPDF\n",
    "        pdfDiffNeg = np.maximum(pdfDiff, z)\n",
    "        minus1 = -1 * pdfDiff\n",
    "        pdfDiffPos = np.maximum(minus1, z)\n",
    "        pdfMax = np.maximum(negPDF, posPDF)\n",
    "        return pdfDiffPos, pdfDiffNeg, pdfMax\n",
    "\n",
    "def _yH2(pdfDiffNeg_NEG, negPDF_NEG, pdfDiffPos_POS, posPDF_POS, posPDF_NEG, negPDF_POS, pdfMax_NEG, pdfMax_POS,p,sampleSize):\n",
    "        numerator1 = np.mean(pdfDiffNeg_NEG / negPDF_NEG)\n",
    "        numerator2 = np.mean(pdfDiffPos_POS / posPDF_POS)\n",
    "        sumVecs = np.power(numerator1, np.ones_like(numerator1) * p) + np.power(numerator2, np.ones_like(numerator2) * p)\n",
    "        dPHat = np.power(sumVecs, np.ones_like(sumVecs) * (1/p))\n",
    "        dTermNeg = (posPDF_NEG * 0.5) + (negPDF_NEG * 0.5)\n",
    "        dTermPos = (posPDF_POS * 0.5) + (negPDF_POS * 0.5)\n",
    "        denominator = (np.sum(pdfMax_NEG / dTermNeg) + np.sum(pdfMax_POS / dTermPos)) / (2 * sampleSize)\n",
    "        return dPHat / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for rep in tqdm(range(10),total=10,desc=\"reps\"):\n",
    "#     # build dataset\n",
    "#     n_epochs = 60\n",
    "#     n_epochs2 = 60\n",
    "#     dsi = initDS(ds_size=50, n_alpha_ests=10)\n",
    "#     # Run gradient method\n",
    "#     maes = g2(dsi,n_epochs=n_epochs)\n",
    "#     # Run agglomerative clustering\n",
    "#     maes2 = g2LL(dsi,n_epochs=n_epochs2)\n",
    "#     # plot results\n",
    "#     fig,ax = plt.subplots(1,5,figsize=(20,4))\n",
    "#     # Plot MAEs\n",
    "#     ax[0].plot(maes,label=\"gradient\")\n",
    "#     ax[1].plot(maes2, label=\"gradient w/ LL\")\n",
    "#     globalMAE = np.mean(np.abs(dsi.trueAlphas - dsi.globalAlphaHats.mean()))\n",
    "#     ax[0].hlines(globalMAE, 0,len(maes),color=\"black\")\n",
    "#     ax[1].hlines(globalMAE, 0,len(maes2),color=\"black\")\n",
    "#     ax[0].legend()\n",
    "#     ax[0].set_title(\"Gradient Method MAE\")\n",
    "#     ax[1].set_title(\"Gradient Method with LL MAE\")\n",
    "#     ax[2].hist(dsi.numP)\n",
    "#     ax[2].set_title(\"Num Positive\")\n",
    "#     ax[3].hist(dsi.numU)\n",
    "#     ax[3].set_title(\"Num Unlabeled\")\n",
    "#     ax[4].hist([h[:n].sum() for h,n in zip(dsi.hiddenLabels, dsi.numU)])\n",
    "#     ax[4].set_title(\"Num Unlabeled Positive\")\n",
    "#     fig.suptitle(\"Distr Distance: {:.4f}    dim:{}\".format(yangDistributionDifference(dsi.posDistMean,dsi.negDistMean,dsi.cov),\n",
    "#                                                           dsi.posDistMean.shape))\n",
    "#     plt.savefig(\"figs/nb_10/fig_{}.pdf\".format(rep),format=\"pdf\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(np.arange(0,5).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initRealDS(filename, size=100,n_alpha_ests=10):\n",
    "    dsi = getRealDS(fileName,size,\n",
    "                    nPDistr=lambda: np.random.choice(np.arange(1,5).astype(int)),\n",
    "                    nUDistr=lambda: np.random.choice(np.arange(20,30).astype(int)),\n",
    "                    alphaDistr=lambda: np.random.uniform(0.05,1))\n",
    "    dsi.alphaHats,dsi.curves = getBagAlphaHats(dsi,numbootstraps=n_alpha_ests)\n",
    "    dsi.globalAlphaHats = getAlphaHat(dsi,reps=n_alpha_ests)\n",
    "    return dsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileNames = glob(\"/home/dzeiberg/ClassPriorEstimation/rawDatasets/*.mat\")[1:]\n",
    "for fileName in tqdm(fileNames, total=len(fileNames),desc=\"reps\"):\n",
    "    name = fileName.split(\"/\")[-1].replace(\".mat\",\"\")\n",
    "    # build dataset\n",
    "    n_epochs = 5000\n",
    "    size = 100\n",
    "    dsi = initRealDS(fileName,size=size,\n",
    "                     n_alpha_ests=10)\n",
    "    # Run gradient method\n",
    "    maes = g2(dsi,n_epochs=n_epochs,eta=.1,regLambda=0.01)\n",
    "    agg = AgglomerativeClustering(dsi, 0.5)\n",
    "    agg.cluster()\n",
    "    aggMAES = agg.meanAbsErrs\n",
    "    fig,ax = plt.subplots(1,6,figsize=(24,4))\n",
    "    # Plot MAEs\n",
    "    ax[0].plot(maes,label=\"gradient\")\n",
    "    ax[1].plot(aggMAES,label=\"clustering\")\n",
    "    globalMAE = np.mean(np.abs(dsi.trueAlphas - dsi.globalAlphaHats.mean()))\n",
    "    ax[0].hlines(globalMAE, 0,len(maes),color=\"black\",label=\"global\")\n",
    "    ax[1].hlines(globalMAE, 0,len(aggMAES),color=\"black\",label=\"global\")\n",
    "    ax[0].set_ylim(0,1)\n",
    "    ax[1].set_ylim(0,1)\n",
    "    ax[0].set_title(\"Gradient Method MAE\")\n",
    "    ax[1].set_title(\"Clustering Method MAE\")\n",
    "    ax[2].hist(dsi.numP)\n",
    "    ax[2].set_title(\"Num Positive\")\n",
    "    ax[3].hist(dsi.numU)\n",
    "    ax[3].set_title(\"Num Unlabeled\")\n",
    "    ax[4].hist([h[:n].sum() for h,n in zip(dsi.hiddenLabels, dsi.numU)])\n",
    "    ax[4].set_title(\"Num Unlabeled Positive\")\n",
    "    ax[5].hist(dsi.trueAlphas)\n",
    "    ax[5].set_title(\"Class Priors\")\n",
    "    plt.suptitle(name)\n",
    "    plt.savefig(\"figs/nb_10/real/reg-2/{}.pdf\".format(name),format=\"pdf\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "number of iterations needed to pleateau seems to scale with the number of bags\n",
    "\n",
    "converging to the global estimate MAE seems to indicate that the local estimates are just becoming the global ones\n",
    "    reducing the number of iterations won't do the trick\n",
    "    adding more weight regularization \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "\n",
    "import scipy.stats as ss\n",
    "\n",
    "from multiinstance.data.realData import buildDataset\n",
    "from multiinstance.utils import *\n",
    "from multiinstance.gradientMethod import g1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = buildDataset(\"/home/dzeiberg/ClassPriorEstimation/rawDatasets/wine.mat\",size=2,\n",
    "                  nPDistr=lambda:np.random.choice([5]),\n",
    "                  nUDistr=lambda:np.random.choice([25]),\n",
    "                 alphaDistr=lambda: np.random.uniform(0.05,.95))\n",
    "\n",
    "ds = addTransformScores(ds)\n",
    "ds = addGlobalEsts(ds)\n",
    "ds = addBagAlphaHats(ds)\n",
    "ds.trueGlobalClassPrior = ds.trueAlphas.flatten().dot(ds.numU) / ds.numU.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(ds):\n",
    "    d = EasyDict()\n",
    "    d.means = ds.alphaHats.mean(1)\n",
    "#     d.variances = ds.alphaHats.var(1)\n",
    "    d.variances = np.ones(ds.N)\n",
    "    d.mixingCoefs = np.ones_like(ds.globalAlphaHats) * .5\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.normal(loc=-2,scale=1,size=100)\n",
    "b = np.random.normal(loc=3,scale=1.5,size=100)\n",
    "ds.globalAlphaHats = np.random.choice(np.concatenate((a,b)),size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(-10,10,.01),ss.norm.pdf(np.arange(-10,10,.01),loc=-2,scale=1))\n",
    "plt.plot(np.arange(-10,10,.01),ss.norm.pdf(np.arange(-10,10,.01),loc=3,scale=1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def E_Step(ds,params):\n",
    "#     N = len(ds.globalAlphaHats)\n",
    "#     K = ds.N\n",
    "#     gamma = np.zeros((N,K))\n",
    "#     for k in range(K):\n",
    "#         mean_k,var_k = params.means[k], params.variances[k]\n",
    "#         for n in range(N):\n",
    "#             print(\"p({:.3f} | {:.3f},{:.3f})={:.3f}\".format(ds.globalAlphaHats[n],mean_k,var_k,\n",
    "#                                              ss.norm.pdf(ds.globalAlphaHats[n],loc=mean_k, scale=var_k)))\n",
    "#             gamma[n,k] = params.mixingCoefs[k] * ss.norm.pdf(ds.globalAlphaHats[n],\n",
    "#                                                              loc=mean_k,\n",
    "#                                                              scale=var_k)\n",
    "#     gamma = gamma / np.tile(gamma.sum(1).reshape((-1,1)),(1,gamma.shape[1]))\n",
    "#     params.gamma = gamma\n",
    "# #     assert False\n",
    "#     return params\n",
    "\n",
    "def E_Step(ds,params):\n",
    "    N = len(ds.globalAlphaHats)\n",
    "    K = ds.N\n",
    "    gamma = np.zeros((N,K))\n",
    "    for n in range(N):\n",
    "        x_n = ds.globalAlphaHats[n]\n",
    "        for k in range(K):\n",
    "            pi_k = params.mixingCoefs[k]\n",
    "            mu_k = params.means[k]\n",
    "            sigma_k = params.variances[k]\n",
    "            pdf_x = ss.norm.pdf(x_n,loc=mu_k, scale=sigma_k)\n",
    "            gamma[n,k]= pi_k * pdf_x\n",
    "        gamma[n] = gamma[n] / gamma[n].sum()\n",
    "    params.gamma = gamma\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def M_Step(ds,params):\n",
    "#     N = len(ds.globalAlphaHats)\n",
    "#     K = ds.N\n",
    "#     Nk = params.gamma.sum(0)\n",
    "#     for k in range(K):\n",
    "#         params.means[k] = params.gamma[:,k].dot(ds.globalAlphaHats) / Nk[k]\n",
    "#         for n in range(N):\n",
    "#             params.variances[k] += params.gamma[n,k] * (ds.globalAlphaHats[n] - params.means[k])**2\n",
    "#         params.variances[k] /= Nk[k]\n",
    "#         params.mixingCoefs[k] = Nk[k] / N\n",
    "#     return params\n",
    "\n",
    "def M_Step(ds,params):\n",
    "    K = ds.N\n",
    "    N = len(ds.globalAlphaHats)\n",
    "    Nk = params.gamma.sum(0)\n",
    "    for k in range(K):\n",
    "        # Mu\n",
    "        for n in range(N):\n",
    "            params.means[k] = params.means[k] + params.gamma[n,k]*ds.globalAlphaHats[n]\n",
    "        params.means[k] = params.means[k] / Nk[k]\n",
    "        # Sigma\n",
    "        for n in range(N):\n",
    "            xn = ds.globalAlphaHats[n]\n",
    "            inc = params.gamma[n,k] * (xn - params.means[k])**2 \n",
    "            params.variances[k] = params.variances[k] + inc\n",
    "        params.variances[k] = params.variances[k] / Nk[k]\n",
    "        # Pi\n",
    "        params.mixingCoefs[k] = Nk[k] / N\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = init(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = []\n",
    "gammas = []\n",
    "variances = []\n",
    "means.append(np.array(params.means.tolist()).reshape((1,-1)))\n",
    "variances.append(np.array(params.variances.tolist()).reshape((1,-1)))\n",
    "for i in range(100):\n",
    "    params = E_Step(ds,params)\n",
    "    params = M_Step(ds,params)\n",
    "    means.append(np.array(params.means.tolist()).reshape((1,-1)))\n",
    "    variances.append(np.array(params.variances.tolist()).reshape((1,-1)))\n",
    "means = np.concatenate(means,axis=0)\n",
    "variances = np.concatenate(variances,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = buildDataset(\"/home/dzeiberg/ClassPriorEstimation/rawDatasets/wine.mat\",size=2,\n",
    "                  nPDistr=lambda:np.random.choice([5]),\n",
    "                  nUDistr=lambda:np.random.choice([25]),\n",
    "                 alphaDistr=lambda: np.random.uniform(0.05,.95))\n",
    "\n",
    "ds = addTransformScores(ds)\n",
    "ds = addGlobalEsts(ds,reps=100)\n",
    "ds = addBagAlphaHats(ds,reps=100)\n",
    "ds.trueGlobalClassPrior = ds.trueAlphas.flatten().dot(ds.numU) / ds.numU.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of points\n",
    "n_samples = 100\n",
    "mu1, sigma1 = ds.trueAlphas[0], .1 # mean and variance\n",
    "mu2, sigma2 = ds.trueAlphas[1], .1 # mean and variance\n",
    "# mu3, sigma3 = ds.trueAlphas[2], .1 # mean and variance\n",
    "\n",
    "x1 = np.random.normal(mu1, np.sqrt(sigma1), n_samples)\n",
    "x2 = np.random.normal(mu2, np.sqrt(sigma2), n_samples)\n",
    "# x3 = np.random.normal(mu3, np.sqrt(sigma3), n_samples)\n",
    "\n",
    "X = np.array(list(x1) + list(x2))\n",
    "X = np.random.choice(ds.globalAlphaHats,size=200)\n",
    "np.random.shuffle(X)\n",
    "print(\"Dataset shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf(data, mean: float, variance: float):\n",
    "    # A normal continuous random variable.\n",
    "    s1 = 1/(np.sqrt(2*np.pi*variance))\n",
    "    s2 = np.exp(-(np.square(data - mean)/(2*variance)))\n",
    "    return s1 * s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the training data\n",
    "bins = np.linspace(0,1,100)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.xlabel(\"$x$\")\n",
    "plt.ylabel(\"pdf\")\n",
    "plt.scatter(X, [0.005] * len(X), color='navy', s=30, marker=2, label=\"Train data\")\n",
    "\n",
    "plt.plot(bins, pdf(bins, mu1, sigma1), color='red', label=\"True pdf\")\n",
    "plt.plot(bins, pdf(bins, mu2, sigma2), color='red')\n",
    "# plt.plot(bins, pdf(bins, mu3, sigma3), color='red')\n",
    "plt.xlim(0,1)\n",
    "plt.legend()\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of clusters to be learned\n",
    "k = 2\n",
    "weights = np.ones((k)) / k\n",
    "means = np.random.choice(X, k)\n",
    "variances = np.random.random_sample(size=k)\n",
    "# print(means, variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps=1e-8\n",
    "for step in range(100):\n",
    "  \n",
    "    if step % 1 == 0:\n",
    "        plt.figure(figsize=(10,6))\n",
    "        axes = plt.gca()\n",
    "        plt.xlabel(\"$x$\")\n",
    "        plt.ylabel(\"pdf\")\n",
    "        plt.title(\"Iteration {}\".format(step))\n",
    "        plt.scatter(X, [0.005] * len(X), color='navy', s=30, marker=2, label=\"Train data\")\n",
    "\n",
    "        plt.plot(bins, pdf(bins, mu1, sigma1), color='grey', label=\"True pdf\")\n",
    "        plt.plot(bins, pdf(bins, mu2, sigma2), color='grey')\n",
    "#         plt.plot(bins, pdf(bins, mu3, sigma3), color='grey')\n",
    "\n",
    "        plt.plot(bins, pdf(bins, means[0], variances[0]), color='blue', label=\"Cluster 1\")\n",
    "        plt.plot(bins, pdf(bins, means[1], variances[1]), color='green', label=\"Cluster 2\")\n",
    "#         plt.plot(bins, pdf(bins, means[2], variances[2]), color='magenta', label=\"Cluster 3\")\n",
    "\n",
    "        plt.legend(loc='upper left')\n",
    "\n",
    "        plt.savefig(\"img_{0:02d}\".format(step), bbox_inches='tight')\n",
    "        plt.show()\n",
    "  \n",
    "    # calculate the maximum likelihood of each observation xi\n",
    "    likelihood = []\n",
    "\n",
    "\n",
    "    # Expectation step\n",
    "    for j in range(k):\n",
    "        likelihood.append(pdf(X, means[j], variances[j]))\n",
    "    likelihood = np.array(likelihood)\n",
    "\n",
    "    b = []\n",
    "    # Maximization step \n",
    "    for j in range(k):\n",
    "        # use the current values for the parameters to evaluate the posterior\n",
    "        # probabilities of the data to have been generanted by each gaussian    \n",
    "        b.append((likelihood[j] * weights[j]) / (np.sum([likelihood[i] * weights[i] for i in range(k)], axis=0)+eps))\n",
    "\n",
    "        # updage mean and variance\n",
    "        means[j] = np.sum(b[j] * X) / (np.sum(b[j]+eps))\n",
    "        variances[j] = np.sum(b[j] * np.square(X - means[j])) / (np.sum(b[j]+eps))\n",
    "\n",
    "        # update the weights\n",
    "        weights[j] = np.mean(b[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.trueAlphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

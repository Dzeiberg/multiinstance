{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp ward_clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from multiinstance.utils import *\n",
    "from multiinstance.distanceApproaches import *\n",
    "from multiinstance.data.syntheticData import buildDataset,getBag\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy.stats as ss\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from numba import set_num_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_num_threads(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class WardClustering:\n",
    "    def __init__(self, dsi,numbootstraps=10,randomPairing=False):\n",
    "        self.randomPairing = randomPairing\n",
    "        self.numbootstraps = numbootstraps\n",
    "        self.clusterAssignment = np.zeros((dsi.N, dsi.N))\n",
    "        self.clusterAssignment[0] = np.arange(dsi.N)\n",
    "        self.clusterAlphaHats = {i: dsi.alphaHats[i] for i in range(dsi.N)}\n",
    "        self.alphaHatMat = np.ones((dsi.N, dsi.N, numbootstraps)) * np.nan\n",
    "        for i in range(dsi.N):\n",
    "            self.alphaHatMat[0,i] = dsi.alphaHats[i]\n",
    "        self.ds = dsi\n",
    "        self.meanAbsErrs = np.zeros(dsi.N)\n",
    "        self.deltas = np.zeros(dsi.N - 1)\n",
    "        self.doLogging(0)\n",
    "        self.log = []\n",
    "        \n",
    "    def doLogging(self, c_iter):\n",
    "#         print(\"logging \",c_iter)\n",
    "        absErrs = []\n",
    "        clusters = np.unique(self.clusterAssignment[c_iter])\n",
    "#         print(self.clusterAssignment)\n",
    "        for ci in clusters:\n",
    "            bags = np.where(self.clusterAssignment[c_iter] == ci)[0]\n",
    "#             print(c_iter, ci)\n",
    "            aHat = self.alphaHatMat[int(c_iter), int(ci)].mean()\n",
    "            alphas = self.ds.trueAlphas[bags].flatten()\n",
    "            # log abs. err for this cluster\n",
    "            aes = np.abs(alphas - aHat)\n",
    "            absErrs.append(aes)\n",
    "        self.meanAbsErrs[c_iter] = np.mean(np.concatenate(absErrs))\n",
    "#         print(\"MAE after \",c_iter,\" merges: \",self.meanAbsErrs)\n",
    "    \n",
    "    def cluster(self):\n",
    "        for c_iter in tqdm(range(1, self.ds.N),desc=\"clustering iter\",total=self.ds.N-1):\n",
    "            clusters = np.unique(self.clusterAssignment[c_iter - 1]).astype(int)\n",
    "            Nc = len(clusters)\n",
    "            deltas = np.ones((Nc, Nc)) * np.inf\n",
    "            alphaHats_Merged_Clusters = np.zeros((Nc,Nc, self.numbootstraps))\n",
    "            if not self.randomPairing:\n",
    "                for i, ci in tqdm(enumerate(clusters), desc=\"ci\", total=Nc, leave=False):\n",
    "                    alphaHat_ci = self.alphaHatMat[c_iter - 1,ci]\n",
    "                    var_ci = np.sum((alphaHat_ci - alphaHat_ci.mean())**2)\n",
    "                    for j, cj in enumerate(set(clusters)):\n",
    "                        if i != j:\n",
    "                            alphaHat_cj = self.alphaHatMat[c_iter-1, cj]\n",
    "                            var_cj = np.sum((alphaHat_cj - alphaHat_cj.mean())**2)\n",
    "                            # alpha hats from i or j\n",
    "                            alphaHats = np.concatenate((alphaHat_ci,\n",
    "                                                           alphaHat_cj))\n",
    "                            # Get alphaHat for joint cluster\n",
    "                            bagIdxs = np.where(np.isin(self.clusterAssignment[c_iter - 1],[ci,cj]))[0]\n",
    "                            alphaHat_cij = self.getClusterEst(bagIdxs)\n",
    "                            alphaHats_Merged_Clusters[i,j] = alphaHat_cij\n",
    "                            var_cij = np.sum((alphaHats - alphaHat_cij.mean())**2)\n",
    "                            deltas[i,j]= var_cij - var_ci - var_cj\n",
    "                # find indices of bags to merge\n",
    "                idx = np.argmin(deltas)\n",
    "                i,j = int(idx / deltas.shape[0]), idx % deltas.shape[0]\n",
    "            else:\n",
    "                i = np.random.choice(np.arange(deltas.shape[0]))\n",
    "                j = np.random.choice(list(set(np.arange(deltas.shape[0])) - set([i])))\n",
    "                ci = clusters[i]\n",
    "                cj = clusters[j]\n",
    "                bagIdxs = np.where(np.isin(self.clusterAssignment[c_iter - 1],[ci,cj]))[0]\n",
    "                alphaHat_cij = self.getClusterEst(bagIdxs)\n",
    "                alphaHats_Merged_Clusters[i,j] = alphaHat_cij\n",
    "                \n",
    "                \n",
    "            \n",
    "            ci, cj = clusters[i], clusters[j]\n",
    "#             print(deltas)\n",
    "            self.log.append((ci,cj))\n",
    "            # deltas i indexed list of increase in cluster variance caused by the i+1_th merge\n",
    "            self.deltas[c_iter - 1] = deltas[i,j]\n",
    "            # set cluster assignment after this merge\n",
    "            self.clusterAssignment[c_iter] = self.clusterAssignment[c_iter - 1]\n",
    "            inI = np.where(self.clusterAssignment[c_iter] == ci)[0]\n",
    "            inJ = np.where(self.clusterAssignment[c_iter] == cj)[0]\n",
    "            self.clusterAssignment[c_iter, inJ] = ci\n",
    "            # update the alphaHat to that estimated from the newly formed cluster\n",
    "#             self.clusterAlphaHats[ci] = alphaHats_Merged_Clusters[i,j]\n",
    "            self.alphaHatMat[c_iter] = self.alphaHatMat[c_iter - 1]\n",
    "            self.alphaHatMat[c_iter, list(set(inI).union(set(inJ)))] = alphaHats_Merged_Clusters[i,j]\n",
    "            self.doLogging(c_iter)\n",
    "\n",
    "    def getClusterEst(self,bagIdxs):\n",
    "        _,U = list(zip(*[getTransformScores(self.ds,b) for b in bagIdxs]))\n",
    "        P, _ = list(zip(*[getTransformScores(self.ds,int(i)) for i in range(self.ds.N)]))\n",
    "        p = np.concatenate(P).reshape((-1,1))\n",
    "        u = np.concatenate(U).reshape((-1,1))\n",
    "        alphaHats, _ = getEsts(p,u,self.numbootstraps)\n",
    "        return alphaHats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsi = buildDataset(2,alphaDistr=lambda: np.random.uniform(.01,.5),\n",
    "                  nP=10,nU=25)\n",
    "\n",
    "dsi = addTransformScores(dsi)\n",
    "\n",
    "dsi.alphaHats,dsi.curves = getBagAlphaHats(dsi,numbootstraps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward = WardClustering(dsi,numbootstraps=5,randomPairing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward.cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward.clusterAssignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward.alphaHatMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(2,1, sharex=True)\n",
    "ax[0].plot(ward.meanAbsErrs)\n",
    "ax[1].plot(np.arange(1, len(ward.deltas) + 1), ward.deltas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

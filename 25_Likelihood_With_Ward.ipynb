{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp tree_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiinstance.ward_clustering import WardClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiinstance.utils import *\n",
    "from multiinstance.data.syntheticData import buildDataset,getBag\n",
    "\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from autograd import grad,hessian\n",
    "from autograd.scipy import  stats as agss\n",
    "import autograd.numpy as np\n",
    "\n",
    "import scipy.stats as ss\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiinstance.data.realData import buildDataset as buildReal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class LikelihoodMethod:\n",
    "    def __init__(self,ds,clusterAssignments, alphaHatMat,lr=0.01,rowLambda=1.0):\n",
    "        self.ds = ds\n",
    "        self.clusterAssignments = clusterAssignments.astype(int)\n",
    "        self.alphaHatMat = alphaHatMat\n",
    "        self.leafMeans = np.mean(self.alphaHatMat[0],axis=1)\n",
    "        self.initClusterVariances()\n",
    "        self.lr = lr\n",
    "        self.meanHistory = []\n",
    "        self.varianceHistory = []\n",
    "        self.MAEs = []\n",
    "        self.NLLs = []\n",
    "        self.rowLambda = rowLambda\n",
    "        \n",
    "    def initClusterVariances(self):\n",
    "            self.clusterVariances = []\n",
    "            # Coordinate tuple in clusterAssignment -> index withing cluster variances\n",
    "            self.loc2Idx = {}\n",
    "            for rowNum in range(self.clusterAssignments.shape[0]):\n",
    "                levelClusters = np.unique(self.clusterAssignments[rowNum])\n",
    "                for cluster in levelClusters:\n",
    "                    self.loc2Idx[(rowNum,cluster)] = len(self.clusterVariances)\n",
    "                    alphaHats= self.alphaHatMat[rowNum, cluster]\n",
    "                    _,v = ss.norm.fit(alphaHats)\n",
    "                    self.clusterVariances.append(v)\n",
    "                    \n",
    "            self.clusterVariances = np.array(self.clusterVariances)\n",
    "                    \n",
    "    def logLikelihood(self,alphaHats, mu, sigma):\n",
    "        LL = np.sum(agss.norm.logpdf(alphaHats, mu, sigma))\n",
    "        LL = LL * (1 / len(alphaHats))\n",
    "        return LL\n",
    "\n",
    "    def treeNegativeLogLikelihood(self):\n",
    "        def getLevelClusters(rowNum):\n",
    "            clusterLabels = np.unique(self.clusterAssignments[rowNum])\n",
    "            clusters = {c : np.where(self.clusterAssignments[rowNum] == c)[0] for c in clusterLabels}\n",
    "            return clusters\n",
    "        \n",
    "        def getClusterMean(leafMeans, clusterMembers):\n",
    "            leafSizes= np.array([self.ds.numU[i] for i in clusterMembers])\n",
    "            alphaTilde = np.dot(leafMeans[clusterMembers], leafSizes) / np.sum(leafSizes)\n",
    "            return alphaTilde\n",
    "            \n",
    "        def NLL(leafMeans, clusterVars):\n",
    "            ll = 0\n",
    "            for rowNum in range(self.clusterAssignments.shape[0]):\n",
    "                clusters = getLevelClusters(rowNum)\n",
    "                for clusterIdx, clusterMembers in clusters.items():\n",
    "                    clusterMean = getClusterMean(leafMeans,clusterMembers)\n",
    "                    varIdx = self.loc2Idx[(rowNum, clusterIdx)]\n",
    "                    clusterVar = clusterVars[varIdx]\n",
    "                    alphaHats = self.alphaHatMat[rowNum, clusterIdx]\n",
    "                    ll = ll + self.rowLambda**rowNum * self.logLikelihood(alphaHats, clusterMean, clusterVar)\n",
    "            return -1 * ll\n",
    "        return NLL\n",
    "\n",
    "    def run(self, n_iters):\n",
    "        gradNLL_mu = grad(self.treeNegativeLogLikelihood(), 0)\n",
    "        gradNLL_sigma = grad(self.treeNegativeLogLikelihood(), 1)\n",
    "        hessianNLL_mu = hessian(self.treeNegativeLogLikelihood(), 0)\n",
    "        hessianNLL_sigma = hessian(self.treeNegativeLogLikelihood(), 1)\n",
    "        self.log()\n",
    "        means = self.leafMeans\n",
    "        var = self.clusterVariances\n",
    "        for iteration in tqdm(range(n_iters)):\n",
    "            if not n_iters % 500:\n",
    "                self.lr *= .95\n",
    "            deltaMu = np.linalg.inv(hessianNLL_mu(means,\n",
    "                                                  var)) @ gradNLL_mu(means,\n",
    "                                                                     var)\n",
    "            deltaSigma = np.linalg.inv(hessianNLL_sigma(mean,\n",
    "                                                        var)) @ gradNLL_sigma(means,\n",
    "                                                                             var)\n",
    "            means = means - self.lr * deltaMu\n",
    "            var = var - self.lr * deltaSigma\n",
    "            assert (self.clusterVariances > 0).all()\n",
    "            self.log()\n",
    "\n",
    "    def log(self):\n",
    "        self.MAEs.append(np.mean(np.abs(dsi.trueAlphas.flatten() - self.leafMeans)))\n",
    "        nllfunc = self.treeNegativeLogLikelihood()\n",
    "        self.NLLs.append(nllfunc(self.leafMeans, self.clusterVariances))\n",
    "        self.meanHistory.append(self.leafMeans)\n",
    "        self.varianceHistory.append(self.clusterVariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDistrTree(trueAlphas, alphaHatMat, meanHistory, scaleHistory,loc2Index,clusterAssignments, numU):\n",
    "    rows,cols = list(zip(*list(method.loc2Idx.keys())))\n",
    "    Nrows = np.max(rows) + 1\n",
    "    Ncols = np.max(cols) + 1\n",
    "    fig,ax = plt.subplots(nrows=Nrows,ncols=Ncols, figsize=(5 * Nrows, 5*Ncols))\n",
    "    for row in range(clusterAssignments.shape[0]):\n",
    "        clusters = np.unique(clusterAssignments[row])\n",
    "        for c in clusters:\n",
    "            scale = scaleHistory[-1][loc2Index[(row,c)]]\n",
    "            scale0 = scaleHistory[0][loc2Index[(row,c)]]\n",
    "            children = np.where(clusterAssignments[row] == c)[0]\n",
    "            childMeans = meanHistory[-1][children]\n",
    "            childMeans0 = meanHistory[0][children]\n",
    "            childN = numU[children]\n",
    "            mu = np.dot(childMeans, childN) / childN.sum()\n",
    "            mu0 = np.dot(childMeans0, childN) / childN.sum()\n",
    "            alpha = np.dot(trueAlphas[children], childN)/ childN.sum()\n",
    "            ax[row,c].plot(np.arange(0,1,.01),\n",
    "                           ss.norm.pdf(np.arange(0,1,.01),loc=mu,scale=scale),color=\"green\")\n",
    "            ax[row,c].plot(np.arange(0,1,.01),\n",
    "                           ss.norm.pdf(np.arange(0,1,.01),loc=mu0,scale=scale0),color=\"red\",alpha=.5)\n",
    "            ax[row,c].hist(alphaHatMat[row,c],density=True,color=\"blue\")\n",
    "            ax[row,c].vlines(alpha, 0,1,color=\"red\")\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def posteriorCorrection(tau, alpha, S0S1):\n",
    "    post =  alpha * S0S1 * (tau / (1 - tau))\n",
    "    post[np.isinf(post)] = 1\n",
    "    return post\n",
    "\n",
    "def correctedAUC(ds,bagAlphaHats,):\n",
    "    _, tauArrays = list(zip(*[getTransformScores(ds,i) for i in range(ds.N)]))\n",
    "    S0_S1 = ds.numU/ds.numP\n",
    "    posteriors = [posteriorCorrection(tau,alphaHat, s0s1) for tau,alphaHat,s0s1 in zip(tauArrays,\n",
    "                                                                                       bagAlphaHats,\n",
    "                                                                                       S0_S1)]\n",
    "    posteriorVals = np.concatenate(posteriors)\n",
    "    hiddenLabels = np.concatenate([ds.hiddenLabels[i][:ds.numU[i]] for i in range(ds.N)])\n",
    "    return roc_auc_score(hiddenLabels, posteriorVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsi = buildDataset(1,alphaDistr=lambda: np.random.choice([.2]),\n",
    "                  nP=100,nU=200,posMean=5,negMean=1,cov=1)\n",
    "ds2 = buildDataset(1,alphaDistr=lambda: np.random.choice([.8]),\n",
    "                  nP=100,nU=200,posMean=5,negMean=1,cov=1)\n",
    "dsi.merge(ds2)\n",
    "dsi = addTransformScores(dsi)\n",
    "dsi = addGlobalEsts(dsi)\n",
    "dsi.alphaHats,dsi.curves = getBagAlphaHats(dsi,numbootstraps=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward = WardClustering(dsi,numbootstraps=dsi.alphaHats.shape[-1],randomPairing=True)\n",
    "ward.cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward.alphaHatMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method = LikelihoodMethod(dsi,ward.clusterAssignment,\n",
    "                          ward.alphaHatMat + np.random.normal(scale=0.00001,size=ward.alphaHatMat.shape),\n",
    "                          lr=0.1,rowLambda=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method.run(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plotDistrTree(dsi.trueAlphas.flatten(),method.alphaHatMat, method.meanHistory,\n",
    "              method.varianceHistory, method.loc2Idx, method.clusterAssignments, dsi.numU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(method.MAEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(method.NLLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "absErrs = {\"local\":[],\n",
    "           \"likelihood\":[],\n",
    "           \"global\": []}\n",
    "aucVals = {\n",
    "    \"local\":[],\n",
    "    \"likelihood\":[],\n",
    "    \"global\":[]\n",
    "}\n",
    "N = 0\n",
    "for f in tqdm(glob(\"/ssdata/ClassPriorEstimationPrivate/data/rawDatasets/*.mat\")):\n",
    "    dsi = buildReal(f,16,\n",
    "                    alphaDistr=lambda: np.random.uniform(.05,.95),\n",
    "                    nPDistr=lambda: 1 + np.random.poisson(125),\n",
    "                    nUDistr=lambda: 1 + np.random.poisson(175))\n",
    "    dsi = addTransformScores(dsi)\n",
    "    dsi = addGlobalEsts(dsi,reps=10)\n",
    "    dsi.alphaHats,dsi.curves = getBagAlphaHats(dsi,\n",
    "                                               numbootstraps=100)\n",
    "    globalMAE = np.mean(np.abs(dsi.trueAlphas.flatten() - dsi.globalAlphaHats.mean()))\n",
    "    absErrs[\"global\"].append(globalMAE * dsi.N)\n",
    "    aucVals[\"local\"].append(correctedAUC(dsi,dsi.alphaHats.mean(1)))\n",
    "    aucVals[\"global\"].append(correctedAUC(dsi,np.ones(dsi.N)*dsi.globalAlphaHats.mean()))\n",
    "    wrd = WardClustering(dsi,numbootstraps=dsi.alphaHats.shape[1],randomPairing=True)\n",
    "    wrd.cluster()\n",
    "    mth = LikelihoodMethod(dsi,wrd.clusterAssignment,\n",
    "                              wrd.alphaHatMat + np.random.normal(scale=0.00001,size=wrd.alphaHatMat.shape),\n",
    "                              lr=0.01,rowLambda=1.0)\n",
    "    mth.run(250)\n",
    "    absErrs[\"local\"].append(mth.MAEs[0] * dsi.N)\n",
    "    absErrs[\"likelihood\"].append(mth.MAEs[-1] * dsi.N)\n",
    "    aucVals[\"likelihood\"].append(correctedAUC(dsi, mth.leafMeans))\n",
    "    maefig,ax = plt.subplots()\n",
    "    \n",
    "    ax.plot(mth.MAEs)\n",
    "    \n",
    "    ax.hlines(globalMAE,0,len(mth.MAEs),label=\"global\")\n",
    "    plt.show()\n",
    "#     treeFig = plotDistrTree(dsi.trueAlphas.flatten(),\n",
    "#                         mth.alphaHatMat,\n",
    "#                         mth.meanHistory,\n",
    "#                         mth.varianceHistory,\n",
    "#                         mth.loc2Idx,\n",
    "#                         mth.clusterAssignments,\n",
    "#                         dsi.numU)\n",
    "#     plt.show()\n",
    "    fig,ax = plt.subplots()\n",
    "    plt.plot(mth.NLLs)\n",
    "    plt.show()\n",
    "    N += dsi.N\n",
    "    print(\"MAE\")\n",
    "    print(\"local: {:.3f}\".format(np.sum(absErrs[\"local\"])/N))\n",
    "    print(\"likelihood: {:.3f}\".format(np.sum(absErrs[\"likelihood\"])/N))\n",
    "    print(\"global: {:.3f}\".format(np.sum(absErrs[\"global\"])/N))\n",
    "    print(\"AUC\")\n",
    "    print(\"local: {:.3f}\".format(np.mean(aucVals[\"local\"])))\n",
    "    print(\"likelihood: {:.3f}\".format(np.mean(aucVals[\"likelihood\"])))\n",
    "    print(\"global: {:.3f}\".format(np.mean(aucVals[\"global\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aucVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

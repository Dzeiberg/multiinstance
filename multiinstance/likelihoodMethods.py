# AUTOGENERATED! DO NOT EDIT! File to edit: 17_Tree_Alpha_Estimate_Distrs.ipynb (unless otherwise specified).

__all__ = ['logLikelihood', 'getChildren', 'treeNegativeLogLikelihood']

# Cell
import autograd
from autograd import grad,jacobian,hessian
from autograd.scipy import stats as agss
import autograd.numpy as np
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

import scipy.stats as ss
import os
from scipy.optimize import minimize
from glob import glob

# Cell
def logLikelihood(xi,mu,sigma,normalize):
    LL = (-len(xi)/2 * np.log(2*np.pi*(sigma + 1e-8)**2) - (1/(2*(sigma + 1e-8)**2)) * np.sum((xi - mu)**2))
    if normalize:
        LL = LL * (1/len(xi))
    return LL

def getChildren(idx,N):
    if idx > N - 1:
        return np.array([idx])
    left = 2 * idx + 1
    right = left + 1

    return np.concatenate([getChildren(left,N),getChildren(right,N)])

def treeNegativeLogLikelihood(x,leafN,normalize=True):
    def LL(leafMeans,bagSigma):
        NBags = len(bagSigma)
        NInternal_Nodes = np.floor(NBags/2)
#         NLeaves = NBags - NInternal_Nodes
        ll = 0
        for idx in range(NBags):
            leafIndices = (getChildren(idx, NInternal_Nodes) - NInternal_Nodes).astype(int)
            ln = leafN[leafIndices]
            mu = np.dot(leafMeans[leafIndices],ln)/np.sum(ln)
            sigma = bagSigma[idx]
            ll = ll + logLikelihood(x[idx],mu,sigma,normalize)
        return -1 * ll
    return LL

# Cell
import autograd
from autograd import grad,jacobian,hessian
from autograd.scipy import stats as agss
import autograd.numpy as np
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import scipy.stats as ss
import os
from scipy.optimize import minimize
from glob import glob

from .likelihoodMethods import *

import scipy.stats as ss

from .data.syntheticData import buildDataset
from .utils import *
from .agglomerative_clustering import AgglomerativeClustering

os.sched_setaffinity(0,set(range(10,20)))
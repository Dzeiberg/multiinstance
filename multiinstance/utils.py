# AUTOGENERATED! DO NOT EDIT! File to edit: 03_Dataset_Utils.ipynb (unless otherwise specified).

__all__ = ['estimator', 'addTransformScores', 'splitIntoBags', 'getTransformScores', 'getBootstrapSample', 'estimate',
           'getEsts', 'getBagAlphaHats', 'getCliqueAlphaHats', 'getAlphaPrime', 'addGlobalEsts', 'addBagAlphaHats',
           'eng', 'path', 'path', 'getKSMatrixPMatrix', 'getAllCliques', 'clusterByLeidenAlg', 'getOptimalAdjacency']

# Cell
from .data.syntheticData import buildDataset
from scipy.stats import ks_2samp
import matplotlib.pyplot as plt
import numpy as np

# import networkx as nx
# import igraph as ig
# import leidenalg
from itertools import chain

from dist_curve.curve_constructor import makeCurve, plotCurve
from dist_curve.transforms import getOptimalTransform
from dist_curve.model import getTrainedEstimator
from tqdm.notebook import tqdm
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import roc_auc_score

# Cell
import os
if os.path.isdir("/ssdata/"):
    pth = "/home/dz/research/ClassPriorEstimation/model.hdf5"
    alphaMaxEstimatorPath = "/home/dz/research/alphamax2/alphamax/estimators/alphamaxEstimator.mat"
elif os.path.isdir("/Users/dan/Documents/"):
    pth = "/Users/dan/Documents/research/ClassPriorEstimation/model.hdf5"
    alphaMaxEstimatorPath = "/Users/dan/Documents/research/AlphaMax/alphamax/estimators/alphamaxEstimator.mat"
else:
    pth = "/data/dzeiberg/ClassPriorEstimation/model.hdf5"
    alphaMaxEstimatorPath = "/home/dzeiberg/alphamax/alphamax/estimators/alphamaxEstimator.mat"
estimator = getTrainedEstimator(pth)

# Cell

def addTransformScores(ds, model=lambda: LogisticRegression(max_iter=1000),n_estimators=100):
    P,U = list(zip(*[ds.getBag(i) for i in range(len(ds.numP))]))

    P = np.concatenate(P)
    U = np.concatenate(U)

    X = np.concatenate((P,U))
    Y = np.concatenate((np.ones(P.shape[0]),
                        np.zeros(U.shape[0])))

#     clf = BaggingClassifier(n_jobs=-1,base_estimator=model(), n_estimators=n_estimators, max_samples=X.shape[0],
#                             max_features=X.shape[1], bootstrap=True, bootstrap_features=False, oob_score=True).fit(X,Y)

#     probP = clf.oob_decision_function_[:,1]

#     ds.aucPU = roc_auc_score(Y, probP)
    transform_scores, auc_pu = getOptimalTransform(X,Y)
    ds.aucPU = auc_pu
    Pprobs, Uprobs = splitIntoBags(transform_scores,ds.numP, ds.numU)
    ds.Pprobs = Pprobs
    ds.Uprobs = Uprobs
    return ds

def splitIntoBags(probs, numP, numU):
    probsP, probsU = probs[:numP.sum()], probs[numP.sum():]
    pUpperIndices = np.concatenate(([0],np.cumsum(numP)))
    uUpperIndices = np.concatenate(([0],np.cumsum(numU)))
    P = np.zeros((len(numP), numP.max()))
    U = np.zeros((len(numU), numU.max()))
    for b in range(len(numP)):
        P[b,:numP[b]] = probsP[pUpperIndices[b]:pUpperIndices[b+1]]
        U[b,:numU[b]] = probsU[uUpperIndices[b] : uUpperIndices[b+1]]
    return P,U

def getTransformScores(ds,i):
    p = ds.Pprobs[i,:ds.numP[i]]
    u = ds.Uprobs[i,:ds.numU[i]]
    return p,u

# Cell
import matlab.engine
eng = matlab.engine.start_matlab()
path = eng.addpath("/home/dzeiberg/alphamax//alphamax/")
path = eng.addpath("/home/dz/research/alphamax2/alphamax/")

def getBootstrapSample(p,u):
    ps = np.random.choice(np.arange(p.shape[0]), size=len(p), replace=True)
    ps = p[ps]
    us = np.random.choice(np.arange(u.shape[0]), size=len(u), replace=True)
    us = u[us]
    return ps, us

def estimate(ps,us, useAlphaMax=False):
    if useAlphaMax:
        est = eng.runAlphaMax(matlab.double(us.tolist()),
                              matlab.double(ps.tolist()),
                              'transform','none',
                              'estimator',alphaMaxEstimatorPath)
        curve = np.zeros(100)
    else:
        curve = makeCurve(ps,us).reshape((1,-1))
        assert curve.sum() > 0
        curve /= curve.sum()
        est = estimator(curve)
    return est,curve

def getEsts(p,u, numbootstraps=10, useAlphaMax=False):
    curves = np.zeros((numbootstraps, 100))
    alphaHats = np.zeros(numbootstraps)
    for i in tqdm(range(numbootstraps),total=numbootstraps,
                  desc="getting distCurve Estimates",leave=False):
        ps, us = getBootstrapSample(p,u)
        alphaHats[i],curves[i] = estimate(ps,us,useAlphaMax=useAlphaMax)
    return alphaHats, curves

# def getBagAlphaHats(ds, numbootstraps=100,useAlphaMax=False):
#     alphaHats =np.zeros((ds.N, numbootstraps))
#     curves =np.zeros((ds.N, numbootstraps, 100))
# #     ps, _ = list(zip(*[ds.getBag(int(i)) for i in range(ds.N)]))
#     ps,_ = list(zip(*[getTransformScores(ds,i) for i in range(ds.N)]))
#     p = np.concatenate(ps).reshape((-1,1))
#     for bagIdx in tqdm(range(ds.N), total=ds.N, desc="getting bag estimates",leave=False):
#         _,u = getTransformScores(ds,bagIdx)
#         u = u.reshape((-1,1))
#         alphaHats[bagIdx], curves[bagIdx] = getEsts(p,u, numbootstraps,useAlphaMax=useAlphaMax)
#     return alphaHats, curves

def getBagAlphaHats(ds, numbootstraps=100, useAlphaMax=False):
    alphaHats = np.zeros((ds.N, numbootstraps))
    curves =np.zeros((ds.N, numbootstraps, 100))
    for bagIdx in tqdm(range(ds.N),total=ds.N, desc="getting bag estimates",leave=False):
        for rep in range(numbootstraps):
            P, U = list(zip(*[getBootstrapSample(*getTransformScores(ds,i)) for i in range(ds.N)]))
            p = np.concatenate(P).reshape((-1,1))
            _, u = getTransformScores(ds,bagIdx)
            u = u.reshape((-1,1))
            u = u[np.random.choice(np.arange(u.shape[0]), size=len(u), replace=True)]
            alphaHats[bagIdx, rep], curves[bagIdx,rep] = estimate(p,u,useAlphaMax=useAlphaMax)
    return alphaHats, curves

def getCliqueAlphaHats(ds, cliques, numbootstraps=10):
    Nc = len(cliques)
    alphaHats = np.zeros((Nc, numbootstraps))
    curves = np.zeros((Nc, numbootstraps, 100))
    for cnum, clique in tqdm(enumerate(cliques), total=Nc, desc="getting clique alpha ests", leave=False):
        _, us = list(zip(*[ds.getBag(int(i)) for i in clique]))
        ps, _ = list(zip(*[ds.getBag(int(i)) for i in range(ds.N)]))
        p = np.concatenate(ps)
        u = np.concatenate(us)
        alphaHats[cnum], curves[cnum] = getEsts(p,u, numbootstraps)
    return alphaHats, curves

def getAlphaPrime(cliques, cliqueEsts):
    bagNums = sorted(set(chain.from_iterable(cliques)))
    alphaPrime = np.zeros(len(bagNums))
    for bn in bagNums:
        inClique = [bn in c for c in cliques]
        alphaPrime[bn] = cliqueEsts[inClique].mean()
    return alphaPrime

def addGlobalEsts(dsi,reps=10, useAlphaMax=False):
    alphaHats = np.zeros(reps)
    for rep in tqdm(range(reps),total=reps,desc="getting global estimates",leave=False):
        P, U = list(zip(*[getBootstrapSample(*getTransformScores(dsi,i)) for i in range(dsi.N)]))
        p = np.concatenate(P).reshape((-1,1))
        u = np.concatenate(U).reshape((-1,1))
        alphaHats[rep],_ = estimate(p,u,useAlphaMax=useAlphaMax)
    dsi.globalAlphaHats = alphaHats
    return dsi

def addBagAlphaHats(dsi,reps=10,useAlphaMax=False):
    alphaHats,curves = getBagAlphaHats(dsi,numbootstraps=reps,useAlphaMax=useAlphaMax)
    dsi.alphaHats = alphaHats
    return dsi

# Cell
def getKSMatrixPMatrix(samples):
    "Get Kolmogrov-Smirnov adjacency matrix from lists of lists of samples for each bag"
    N = samples.shape[0]
    pmat = np.zeros((N,N))
    for bag0Idx in tqdm(range(N),total=N, desc="making kolmogorov-smirnov adj matrix", leave=False):
        for bag1Idx in range(bag0Idx+ 1,N):
            stat,p = ks_2samp(samples[bag0Idx], samples[bag1Idx])
            pmat[bag0Idx, bag1Idx] = p
            pmat[bag1Idx, bag0Idx] = p
    return pmat

def getAllCliques(mat, cutoffval=0.05):
    """
    given matrix of pairwise test p-values,
    make adjacency matrix using specified
    confidence level then find all cliques for each bag
    """
    adj = mat > cutoffval
    g = nx.from_numpy_array(adj)
    return list(nx.algorithms.clique.find_cliques(g))

def clusterByLeidenAlg(similarityMatrix, resolution_parameter = 1.5):
    """
    https://medium.com/@ciortanmadalina
    This method partitions input data by applying the Leiden algorithm
    on a given distance matrix.
    """
    # convert distance matrix to similariy matrix
    distanceMatrix = similarityMatrix
    edges = np.unravel_index(np.arange(distanceMatrix.shape[0]*distanceMatrix.shape[1]), distanceMatrix.shape)
    edges = list(zip(*edges))
    weights = distanceMatrix.ravel()

    g = ig.Graph(directed=False)
    g.add_vertices(distanceMatrix.shape[0])  # each observation is a node
    g.add_edges(edges)

    g.es['weight'] = weights
    weights = np.array(g.es["weight"]).astype(np.float64)
    partition_type = leidenalg.RBConfigurationVertexPartition
    partition_kwargs = {}
    partition_kwargs["weights"] = weights
    partition_kwargs["resolution_parameter"] = resolution_parameter
    part = leidenalg.find_partition(g, partition_type, **partition_kwargs)
    groupAssignment = np.array(part.membership)
    groups = [np.where(groupAssignment==g)[0] for g in np.unique(groupAssignment)]
    return groups

# Cell
def getOptimalAdjacency(trueAlphas):
    N = trueAlphas.shape[0]
    adj = np.zeros((N,N))
    for i,a0 in enumerate(trueAlphas):
        for j,a1 in enumerate(trueAlphas[i+1:],start=i+1):
            adj[i,j] = np.abs(a0 - a1)
            adj[j,i] = np.abs(a0 - a1)
    return adj
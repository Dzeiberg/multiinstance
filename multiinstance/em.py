# AUTOGENERATED! DO NOT EDIT! File to edit: 33_EM.ipynb (unless otherwise specified).

__all__ = ['KMeansInit', 'EM', 'MultiEM', 'mvn_gen', 'generateBags']

# Cell
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import scipy.stats as ss
from easydict import EasyDict
from scipy.spatial.distance import cdist
from sklearn.metrics import roc_auc_score,adjusted_mutual_info_score, adjusted_rand_score
from tqdm.notebook import tqdm

# Cell
def KMeansInit(X, k):
    means = [X[np.random.choice(np.arange(X.shape[0]))]]
    for iteration in range(k-1):
        dists = cdist(X, np.stack(means)).min(1)
        probs = dists / dists.sum()
        nextIndex = np.random.choice(np.arange(X.shape[0]),p=probs)
        means.append(X[nextIndex])
    return np.stack(means)

class EM:
    def __init__(self, X, k):
        self.X = X
        self.k = k
        self.N,self.dim = self.X.shape
        self.initializeComponents()

    def initializeComponents(self):
        mins = self.X.min(0)
        maxs = self.X.max(0)
        # USE K-MEANS++
#         self.mus = KMeansInit(self.X, self.k)
        self.mus = self.X[np.random.choice(np.arange(self.X.shape[0]),size=self.k)]
        self.covs = np.stack([np.eye(self.dim) for _ in range(self.k)])
        self.pi = np.random.dirichlet(np.ones(self.k))

    def E_Step(self):
        gammas = np.zeros((self.N,self.k))
        for n in range(self.N):
            for k in range(self.k):
                gammas[n,k] = self.pi[k] * ss.multivariate_normal.pdf(self.X[n],
                                                                      mean=self.mus[k],
                                                                      cov=self.covs[k])
            gammas[n] = gammas[n] / gammas[n].sum()
        self.gammas = gammas

    def M_Step(self):
        Nk = np.sum(self.gammas, axis=0)[:,np.newaxis]
        self.mus = np.dot(self.gammas.T, self.X) / Nk
        self.pi = np.mean(self.gammas,axis=0)
        for k in range(self.k):
            x = np.matrix(self.X - self.mus[k,:])
            gamma_diag = np.matrix(np.diag(self.gammas[:,k]))
            sigma_k = x.T * gamma_diag * x
            self.covs[k,:,:] = sigma_k / Nk[k]


    def log_likelihood(self):
        ll = 0
        for xn in self.X:
            ll += np.log(np.sum([pik * ss.multivariate_normal.pdf(xn,
                                                                  mean=muk,
                                                                  cov=covk) for pik,muk,covk in zip(self.pi,
                                                                                                    self.mus,
                                                                                                    self.covs)]))
        return ll

# Cell
class MultiEM:
    def __init__(self, bags, n_components):
        self.em_instances = []
        self.labels = []
        self.bags = bags
        # Initialize each EM instance
        for bag in self.bags:
            self.em_instances.append(EM(bag.X_pos, n_components))
        # Override the initialization

    def run_positive_em(self, n_iters, multi=True, KMeansPPInit=True):
        if KMeansPPInit:
            if multi:
                for inst in range(len(self.em_instances)):
                    self.em_instances[inst].mus = KMeansInit(np.concatenate([e.X for e in self.em_instances]),
                                                      self.em_instances[0].k)
            else:
                self.em_instances[0].mus = KMeansInit(np.concatenate([e.X for e in self.em_instances]),
                                                      self.em_instances[0].k)
        self.logLikelihoods = np.zeros((len(self.em_instances),
                                        n_iters+2))
        for inst in range(len(self.em_instances)):
            self.logLikelihoods[inst,0] = self.em_instances[inst].log_likelihood()
        for iteration in tqdm(range(1, n_iters+1),total=n_iters,leave=False):
            for inst in range(len(self.em_instances)):
                self.em_instances[inst].E_Step()
                self.em_instances[inst].M_Step()
                if multi:
                    self.em_instances[(inst+1) % len(self.em_instances)].mus = self.em_instances[inst].mus
                    self.em_instances[(inst+1) % len(self.em_instances)].covs = self.em_instances[inst].covs
                self.logLikelihoods[inst,iteration] = self.em_instances[inst].log_likelihood()
        # After final iteration, pass mu and cov to all instances and update gammas through the E_Step
        for j in range(len(self.em_instances)):
            if multi:
                self.em_instances[j].mus = self.em_instances[0].mus
                self.em_instances[j].covs = self.em_instances[0].covs
                # update gamma with new mus and covs values
                self.em_instances[j].E_Step()
            self.logLikelihoods[j, -1] = self.em_instances[j].log_likelihood()
        self.compute_positive_clustering_performance()

    def compute_positive_clustering_performance(self):
        adjusted_rand_scores = []
        for em,bag in zip(self.em_instances, self.bags):
            adjusted_rand_scores.append(adjusted_rand_score(bag.positive_component_labels,
                                                            np.argmax(em.gammas,axis=1)))
        self.score = np.mean(adjusted_rand_scores)

    def cluster_unlabeled(self):
        for bagnum, (bag, em) in enumerate(zip(self.bags, self.em_instances)):
            assignments = np.zeros(bag.x_unlabeled.shape[0]).astype(int)
            for instnum,inst in enumerate(bag.x_unlabeled):
                lls = np.zeros(em.k)
                for i,(mu,cov) in enumerate(zip(em.mus, em.covs)):
                    lls[i] = ss.multivariate_normal.logpdf(inst,mean=mu,cov=cov)
                assignments[instnum] = np.argmax(lls)
            self.bags[bagnum].unlabeled_assignments = assignments

# Cell
def mvn_gen(mean):
    return lambda n: np.random.multivariate_normal(mean, np.eye(len(mean)),size=n)

def generateBags(NBags,
                 NPos=200,
                 NUnlabeled=1000,
                 pos_means=[[2,2], [-2,-2], [2,-2]],
                 neg_means=[[0,0]]):
    bags = []
    pos_components = [mvn_gen(m) for m in pos_means]
    neg_components = [mvn_gen(m) for m in neg_means]
    for _ in range(NBags):
        pos_weights = np.random.dirichlet(np.ones(len(pos_components))*5)
        bag = EasyDict()
        bag.pi = pos_weights
        bag.pos_weights = pos_weights
        x = []
        positive_component_labels = []
        for i,(comp,w) in enumerate(zip(pos_components,pos_weights)):
            ni = np.round(NPos * w).astype(int)
            x.append(comp(ni))
            positive_component_labels.append(np.ones(ni) * i)
        bag.X_pos = np.concatenate(x)
        bag.positive_component_labels = np.concatenate(positive_component_labels)
        alpha = np.random.beta(2,2)
        bag.alpha = alpha
        n_unlabeled_pos = np.round(NUnlabeled * alpha).astype(int)
        n_unlabeled_neg = NUnlabeled - n_unlabeled_pos
        xunlabeled = []
        unlabeled_pos_componenet_labels = []
        for i,(comp,w) in enumerate(zip(pos_components,pos_weights)):
            ni = np.round(n_unlabeled_pos * w).astype(int)
            xunlabeled.append(comp(ni))
            unlabeled_pos_componenet_labels.append(np.ones(ni) * i)
        bag.unlabeled_pos_componenet_labels = unlabeled_pos_componenet_labels
        unlabeled_neg_weights = np.random.dirichlet(np.ones(len(neg_components)) * 5)
        bag.rho = unlabeled_neg_weights
        unlabeled_neg_componenet_labels = []
        for i, (comp,w) in enumerate(zip(neg_components, unlabeled_neg_weights)):
            ni = np.round(n_unlabeled_neg * w).astype(int)
            xunlabeled.append(comp(ni))
            unlabeled_neg_componenet_labels.append(np.ones(ni) * i)
        bag.unlabeled_neg_componenet_labels = unlabeled_neg_componenet_labels
        bag.x_unlabeled = np.concatenate(xunlabeled)
        bags.append(bag)
    return bags
# AUTOGENERATED! DO NOT EDIT! File to edit: 09_Autograd.ipynb (unless otherwise specified).

__all__ = ['getAlphaLoss', 'getWLoss', 'getAlphaHat', 'gradientMethod']

# Cell
from tqdm.notebook import tqdm

import autograd.numpy as np
from autograd import grad

import matplotlib.pyplot as plt


from .dataset_utils import buildDataset
from .utils import *
from .distanceApproaches import *
from .agglomerative_clustering import AgglomerativeClustering
from numba import set_num_threads

# Cell

def getAlphaLoss(w,n, alphaHats):
    def loss(alpha):
        lossVal = 0
        for wi, aH in zip(w, alphaHats):
            tilde = (1 / np.dot(wi,n)) * np.dot(np.multiply(alpha,wi),n)
            lossVal += .5 * np.square(aH - tilde)
        return lossVal
    return loss

def getWLoss(a,n, alphaHats):
    def loss(w):
        lossVal = 0
        for wi,aH in zip(w, alphaHats):
            tilde = (1 / np.dot(wi,n)) * np.dot(np.multiply(a,wi),n)
            lossVal += .5 * np.square(aH - tilde)
        return lossVal
    return loss

# Cell
def getAlphaHat(dsi,reps=10):
    P, U = list(zip(*[dsi.getBag(int(i)) for i in range(dsi.N)]))
    p = np.concatenate(P)
    u = np.concatenate(U)
    alphaHats,_ = getEsts(p,u,reps)
    return alphaHats


# Cell
def gradientMethod(dsi, n_epochs=100):
    alphaHats = dsi.globalAlphaHats
    # initialize values for gradient method
    a = dsi.alphaHats.mean(1)
    n = dsi.numU
    w = np.random.uniform(low=0.01,high=1,size=(len(alphaHats),
                                                n.shape[0]))
    maes = [np.mean(np.abs(a - dsi.trueAlphas.flatten()))]
    # Run gradient method
    for i in tqdm(range(n_epochs),total=n_epochs):
        alphaLossFn = getAlphaLoss(w,n,alphaHats)
        alphaGrad = grad(alphaLossFn)
        a = a - .025 * alphaGrad(a)
        wLossFn = getWLoss(a,n,alphaHats)
        wGrad = grad(wLossFn)
        w = w - .025 * wGrad(w)
        maes.append(np.mean(np.abs(a - dsi.trueAlphas.flatten())))
    return maes
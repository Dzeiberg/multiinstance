# AUTOGENERATED! DO NOT EDIT! File to edit: 12_Gradient_With_Size_Penalty.ipynb (unless otherwise specified).

__all__ = ['getAlphaHat', 'getAlphaLoss', 'getWLoss', 'gradientMethod', 'getAlphaHat', 'getAlphaLoss',
           'getAlphaLossWithLL', 'getWLoss', 'getGlobalAlphaHat', 'initDS', 'addEsts', 'aL0', 'wL0', 'g1',
           'plotResults', 'initRealDS']

# Cell
from tqdm.notebook import tqdm

import autograd.numpy as np
from autograd import grad
import autograd.scipy.stats as agss

import matplotlib.pyplot as plt


from .dataset_utils import buildDataset
from .utils import *
from .distanceApproaches import *
from .agglomerative_clustering import AgglomerativeClustering
from numba import set_num_threads

import scipy.stats as ss

# Cell
def getAlphaHat(dsi,reps=10):
    P, U = list(zip(*[dsi.getBag(int(i)) for i in range(dsi.N)]))
    p = np.concatenate(P)
    u = np.concatenate(U)
    alphaHats,_ = getEsts(p,u,reps)
    return alphaHats


# Cell

def getAlphaLoss(w,n, alphaHats):
    def loss(alpha):
        lossVal = 0
        for wi, aH in zip(w, alphaHats):
            tilde = (1 / np.dot(wi,n)) * np.dot(np.multiply(alpha,wi),n)
            lossVal = lossVal + .5 * np.square(aH - tilde)
        return lossVal
    return loss

def getWLoss(a,n, alphaHats):
    def loss(w):
        lossVal = 0
        for wi,aH in zip(w, alphaHats):
            den = (1 / np.dot(wi,n))
            aXw = np.multiply(a,wi)
            dot = np.dot(aXw,n)
            tilde =  den * dot
            lossVal = lossVal + .5 * np.square(aH - tilde)
        return lossVal
    return loss

# Cell
def gradientMethod(dsi, n_epochs=100):
    alphaHats = dsi.globalAlphaHats
    # initialize values for gradient method
    a = dsi.alphaHats.mean(1)
    n = dsi.numU
    w = np.random.uniform(low=0.01,high=1,size=(len(alphaHats),
                                                n.shape[0]))
    maes = [np.mean(np.abs(a - dsi.trueAlphas.flatten()))]
    # Run gradient method
    for i in tqdm(range(n_epochs),total=n_epochs):
        alphaLossFn = getAlphaLoss(w,n,alphaHats)
        alphaGrad = grad(alphaLossFn)
        a = a - .025 * alphaGrad(a.flatten()).reshape(a.shape)
        wLossFn = getWLoss(a,n,alphaHats)
        wGrad = grad(wLossFn)
        w = w - .025 * wGrad(w)
        maes.append(np.mean(np.abs(a - dsi.trueAlphas.flatten())))
    return maes

# Cell
from tqdm.notebook import tqdm

import autograd.numpy as np
from autograd import grad
import autograd.scipy.stats as agss

import matplotlib.pyplot as plt


from .dataset_utils import buildDataset
from .utils import *
from .distanceApproaches import *
from .agglomerative_clustering import AgglomerativeClustering
from numba import set_num_threads

import scipy.stats as ss
from .data.realData import buildDataset as getRealDS

from glob import glob

# Cell
def getAlphaHat(dsi,reps=10):
    P, U = list(zip(*[dsi.getBag(int(i)) for i in range(dsi.N)]))
    p = np.concatenate(P)
    u = np.concatenate(U)
    alphaHats,_ = getEsts(p,u,reps)
    return alphaHats


# Cell

def getAlphaLoss(w,n, alphaHats):
    def loss(localAlphaHats):
        lossVal = 0
        for wi, aH in zip(w, alphaHats):
            tilde = (1 / np.dot(wi,n)) * np.dot(np.multiply(localAlphaHats,wi),n)
            lossVal = lossVal + .5 * np.square(aH - tilde)
        return lossVal
    return loss

def getAlphaLossWithLL(w,n, alphaHats,aMLEVals, bMLEVals, locMLE, scaleMLE):
    def loss(localAlphaHats):
        lossVal = 0
        for wi, aH in zip(w, alphaHats):
            tilde = (1 / np.dot(wi,n)) * np.dot(np.multiply(localAlphaHats,wi),n)
            lossVal = lossVal + .5 * np.square(aH - tilde)
        # add negative log-likelihood for each local bag to loss
        for aHats, aMLE, bMLE,loc, scale in zip(localAlphaHats, aMLEVals,
                                     bMLEVals, locMLE, scaleMLE):
            lossVal = lossVal - 1e-3 * agss.beta.pdf((aHats - loc)/scale, a=aMLE, b=bMLE).sum()
        return lossVal
    return loss

def getWLoss(a,n, alphaHats, regLambda=1e-5):
    def loss(w):
        lossVal = 0
        for wi,aH in zip(w, alphaHats):
            den = (1 / np.dot(wi,n))
            aXw = np.multiply(a,wi)
            dot = np.dot(aXw,n)
            tilde =  den * dot
            lossVal = lossVal + .5 * np.square(aH - tilde)
        lossVal = lossVal + regLambda * np.linalg.norm(w)
        return lossVal
    return loss

# Cell
from tqdm.notebook import tqdm

import autograd.numpy as np
from autograd import grad
import autograd.scipy.stats as agss

import matplotlib.pyplot as plt


from .dataset_utils import buildDataset
from .utils import *
from .distanceApproaches import *
from .agglomerative_clustering import AgglomerativeClustering
from numba import set_num_threads

import scipy.stats as ss
from .data.realData import buildDataset as getRealDS

from glob import glob

import scipy.stats as ss

# Cell
def getGlobalAlphaHat(dsi,reps=10):
    alphaHats = np.zeros(reps)
    for rep in tqdm(range(reps),total=reps,desc="getting global estimates"):
        P, U = list(zip(*[getBootstrapSample(*dsi.getBag(int(i))) for i in range(dsi.N)]))
        p = np.concatenate(P)
        u = np.concatenate(U)
        alphaHats[rep],_ = estimate(p,u)
    return alphaHats


def initDS(ds_size=100,n_alpha_ests=50, nP=None, nU=None,
           alphaDistr=lambda: np.random.uniform(0.1,.5),posMean=None, negMean=None,cov=None):
    dsi = buildDataset(ds_size,alphaDistr=alphaDistr, nP=nP,
                       nU=nU,posMean=posMean, negMean=negMean,cov=cov)
    return addEsts(dsi)

def addEsts(dsi,n_alpha_ests=10):
    dsi.alphaHats,dsi.curves = getBagAlphaHats(dsi,numbootstraps=n_alpha_ests)
    dsi.alphaHats = dsi.alphaHats.reshape((-1,n_alpha_ests))
    dsi.globalAlphaHats = getGlobalAlphaHat(dsi,reps=n_alpha_ests)
    return dsi

# Cell
def aL0(w,n,globalAlphaHats,a0,varLambda=.1, anchorLambda=.1):
    def loss(localAlphaHats):
        lossVal = 0
#         localAlphaHats = 1 / (1 + np.exp(-1 * localAlphaHats))
        for wi, aH in zip(w, globalAlphaHats):
            tilde = 1 / np.sum(np.multiply(n,wi))
            wiXA = np.multiply(wi,localAlphaHats)
            tilde = tilde * np.sum(np.multiply(wiXA,
                                               n))
            lossVal = lossVal + .5 * np.square(aH - tilde)
        lossVal = lossVal + varLambda * np.sum(np.var(localAlphaHats,axis=1))
        lossVal = lossVal + anchorLambda * np.sum(np.square(localAlphaHats - a0))
        return lossVal
    return loss

def wL0(localAlphaHats, n, globalAlphaHats,regLambda=0, wOneLambda=0):
#     localAlphaHats = 1 / (1 + np.exp(-1 * localAlphaHats))
    def loss(w):
        lossVal = 0
        for wi,aH in zip(w, globalAlphaHats):
            den = 1 / np.sum(np.multiply(n,wi))
            wiXA = np.multiply(wi,localAlphaHats)
            dot = np.sum(np.multiply(wiXA,n))
            tilde =  den * dot
            lossVal = lossVal + .5 * np.square(aH - tilde)
            # The weights across all local estimates for each global estimate should sum to 1
            lossVal = lossVal + wOneLambda * .5 * np.sum(np.square(wi - 1))
        lossVal = lossVal + regLambda * np.linalg.norm(w)
        return lossVal
    return loss

def g1(dsi, n_epochs=100,eta=1,varLambda=.1, anchorLambda=.1, regLambda=1e-5, wOneLambda=1):
    NBags = dsi.numU.shape[0]
    globalAlphaHats = dsi.globalAlphaHats
    # initialize values for gradient method
    a = dsi.alphaHats
    a0 = dsi.alphaHats
    n = np.tile(dsi.numU.reshape((-1,1)), (1,a.shape[1]))
    w = np.random.uniform(low=0.01, high=1,size=(len(globalAlphaHats),n.shape[0],n.shape[1]))
    maes = [np.mean(np.abs(a.mean(1) - dsi.trueAlphas.flatten()))]
    for i in tqdm(range(n_epochs), total=n_epochs,leave=False):
        # A iteration
        alphaLossFn = aL0(w,n,globalAlphaHats,a0,varLambda=varLambda, anchorLambda=anchorLambda)
        alphaGrad = grad(alphaLossFn)
        a = a - eta * alphaGrad(a)
        a = np.maximum(np.zeros_like(a),np.minimum(a,np.ones_like(a)))
        # W iteration
        wLossFn = wL0(a,n,globalAlphaHats,regLambda=regLambda, wOneLambda=wOneLambda)
        wGrad = grad(wLossFn)
        w = w - eta * wGrad(w)
        maes.append(np.mean(np.abs(a.mean(1) - dsi.trueAlphas.flatten())))
    return {"maes":maes,
            "alphaHats": a,
            "weights": w,
           "baseline_mae": np.mean(np.abs(dsi.trueAlphas.flatten() - globalAlphaHats.mean()))}

# Cell
def plotResults(dsi,res):
    # plot results
    fig,ax = plt.subplots(1,2,figsize=(12,4))
    # Plot MAEs
    ax[0].plot(res["maes"],label="gradient")
    # add global baseline
    globalMAE = np.mean(np.abs(dsi.trueAlphas - dsi.globalAlphaHats.mean()))
    ax[0].hlines(globalMAE, 0,len(res["maes"]),color="black",label="global")
    ax[0].legend()
    ax[0].set_title("Gradient Method MAE")
    # Plot final alphaHat
    N = len(dsi.numU)
    K = len(dsi.globalAlphaHats)
    for i in range(N):
        ax[1].fill_between(np.array([res["alphaHats"][i].min(),
                                     res["alphaHats"][i].max()]),
                           y1=0,
                           y2=dsi.numU[i]+.25,
                            alpha=.25,color="red")
        ax[1].vlines(res["alphaHats"][i].mean(),0,dsi.numU[i]+1.5,color="red")
    ax[1].vlines(dsi.globalAlphaHats.mean(),
                 0,
                 max(dsi.numU),
                 color="black",label=r"$\hat{\alpha_{c_i}}$")
    ax[1].fill_between(np.array([
        dsi.globalAlphaHats.min(),
        dsi.globalAlphaHats.max()]),
    y1=0,y2=np.max(dsi.numU),color="black",alpha=.25)

    for i in range(N):
        ax[1].fill_between(np.array([
            dsi.alphaHats[i].min(),
            dsi.alphaHats[i].max()
        ]), y1=0,y2=dsi.numU[i],color="blue",alpha=.25)
    ax[1].vlines(dsi.alphaHats.mean(1),
                 0,
                 dsi.numU-.15,
                 color="blue",label=r"$\hat{\alpha}_0$")
    ax[1].vlines(dsi.trueAlphas,0,dsi.numU - .25,color="green",label=r"$\alpha$")
    ax[1].vlines(dsi.trueGlobalClassPrior,0,dsi.numU.max(),color="orange",label=r"$\alpha_c$")
    ax[1].set_title("Alphas")
#     ax[1].set_xlim(0,1)
    ax[1].legend(loc="upper right", bbox_to_anchor=(1.25, 1))
    # plot weights
    #ax[2].vlines(res["weights"],0,np.tile(dsi.numU,(K,1)))
    plt.show()

# Cell
def initRealDS(fileName, size=100,n_alpha_ests=10):
    dsi = getRealDS(fileName,size,
                    nPDistr=lambda: np.random.choice(np.arange(1,5).astype(int)),
                    nUDistr=lambda: np.random.choice(np.arange(20,30).astype(int)),
                    alphaDistr=lambda: np.random.uniform(0.05,1))
    return addEsts(dsi)
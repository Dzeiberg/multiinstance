# AUTOGENERATED! DO NOT EDIT! File to edit: 10_Gradient_With_Likelihood.ipynb (unless otherwise specified).

__all__ = ['getAlphaHat', 'getAlphaLoss', 'getWLoss', 'gradientMethod', 'getAlphaHat', 'getAlphaLoss',
           'getAlphaLossWithLL', 'getWLoss']

# Cell
from tqdm.notebook import tqdm

import autograd.numpy as np
from autograd import grad
import autograd.scipy.stats as agss

import matplotlib.pyplot as plt


from .dataset_utils import buildDataset
from .utils import *
from .distanceApproaches import *
from .agglomerative_clustering import AgglomerativeClustering
from numba import set_num_threads

import scipy.stats as ss

# Cell
def getAlphaHat(dsi,reps=10):
    P, U = list(zip(*[dsi.getBag(int(i)) for i in range(dsi.N)]))
    p = np.concatenate(P)
    u = np.concatenate(U)
    alphaHats,_ = getEsts(p,u,reps)
    return alphaHats


# Cell

def getAlphaLoss(w,n, alphaHats):
    def loss(alpha):
        lossVal = 0
        for wi, aH in zip(w, alphaHats):
            tilde = (1 / np.dot(wi,n)) * np.dot(np.multiply(alpha,wi),n)
            lossVal = lossVal + .5 * np.square(aH - tilde)
        return lossVal
    return loss

def getWLoss(a,n, alphaHats):
    def loss(w):
        lossVal = 0
        for wi,aH in zip(w, alphaHats):
            den = (1 / np.dot(wi,n))
            aXw = np.multiply(a,wi)
            dot = np.dot(aXw,n)
            tilde =  den * dot
            lossVal = lossVal + .5 * np.square(aH - tilde)
        return lossVal
    return loss

# Cell
def gradientMethod(dsi, n_epochs=100):
    alphaHats = dsi.globalAlphaHats
    # initialize values for gradient method
    a = dsi.alphaHats.mean(1)
    n = dsi.numU
    w = np.random.uniform(low=0.01,high=1,size=(len(alphaHats),
                                                n.shape[0]))
    maes = [np.mean(np.abs(a - dsi.trueAlphas.flatten()))]
    # Run gradient method
    for i in tqdm(range(n_epochs),total=n_epochs):
        alphaLossFn = getAlphaLoss(w,n,alphaHats)
        alphaGrad = grad(alphaLossFn)
        a = a - .025 * alphaGrad(a.flatten()).reshape(a.shape)
        wLossFn = getWLoss(a,n,alphaHats)
        wGrad = grad(wLossFn)
        w = w - .025 * wGrad(w)
        maes.append(np.mean(np.abs(a - dsi.trueAlphas.flatten())))
    return maes

# Cell
from tqdm.notebook import tqdm

import autograd.numpy as np
from autograd import grad
import autograd.scipy.stats as agss

import matplotlib.pyplot as plt


from .dataset_utils import buildDataset
from .utils import *
from .distanceApproaches import *
from .agglomerative_clustering import AgglomerativeClustering
from numba import set_num_threads

import scipy.stats as ss

# Cell
def getAlphaHat(dsi,reps=10):
    P, U = list(zip(*[dsi.getBag(int(i)) for i in range(dsi.N)]))
    p = np.concatenate(P)
    u = np.concatenate(U)
    alphaHats,_ = getEsts(p,u,reps)
    return alphaHats


# Cell

def getAlphaLoss(w,n, alphaHats):
    def loss(localAlphaHats):
        lossVal = 0
        for wi, aH in zip(w, alphaHats):
            tilde = (1 / np.dot(wi,n)) * np.dot(np.multiply(localAlphaHats,wi),n)
            lossVal = lossVal + .5 * np.square(aH - tilde)
        return lossVal
    return loss

def getAlphaLossWithLL(w,n, alphaHats,aMLEVals, bMLEVals, locMLE, scaleMLE):
    def loss(localAlphaHats):
        lossVal = 0
        for wi, aH in zip(w, alphaHats):
            tilde = (1 / np.dot(wi,n)) * np.dot(np.multiply(localAlphaHats,wi),n)
            lossVal = lossVal + .5 * np.square(aH - tilde)
        # add negative log-likelihood for each local bag to loss
        for aHats, aMLE, bMLE,loc, scale in zip(localAlphaHats, aMLEVals,
                                     bMLEVals, locMLE, scaleMLE):
            lossVal = lossVal - 1e-3 * agss.beta.pdf((aHats - loc)/scale, a=aMLE, b=bMLE).sum()
        return lossVal
    return loss

def getWLoss(a,n, alphaHats):
    def loss(w):
        lossVal = 0
        for wi,aH in zip(w, alphaHats):
            den = (1 / np.dot(wi,n))
            aXw = np.multiply(a,wi)
            dot = np.dot(aXw,n)
            tilde =  den * dot
            lossVal = lossVal + .5 * np.square(aH - tilde)
        return lossVal
    return loss